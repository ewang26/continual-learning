{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean gradient similarity: 0.0263\n",
      "Standard deviation of gradient similarity: 0.0190\n"
     ]
    }
   ],
   "source": [
    "#checking mean and similarity on 50 random models (mlp)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Import the MNIST model from models.py\n",
    "from models import MNISTNet\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to initialize a model with a given seed\n",
    "def init_model(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    return MNISTNet().to(device)\n",
    "\n",
    "# Function to calculate gradient similarity between two models\n",
    "def calculate_gradient_similarity(model1, model2):\n",
    "    # Create dummy data\n",
    "    dummy_input = torch.randn(100, 3, 28, 28).to(device)\n",
    "    dummy_target = torch.randint(0, 10, (100,)).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    # Calculate gradients\n",
    "    grad1 = get_gradients_simple(dummy_input, dummy_target, model1, criterion)\n",
    "    grad2 = get_gradients_simple(dummy_input, dummy_target, model2, criterion)\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarity = np.dot(grad1, grad2) / (np.linalg.norm(grad1) * np.linalg.norm(grad2))\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def get_gradients_simple(X, y, model, criterion):\n",
    "    model.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y).mean()  # Use mean reduction\n",
    "    loss.backward()\n",
    "    \n",
    "    grad_list = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_list.append(p.grad.detach().cpu().numpy().flatten())\n",
    "    \n",
    "    return np.concatenate(grad_list)\n",
    "\n",
    "# Run the experiment\n",
    "n_pairs = 50\n",
    "similarities = []\n",
    "\n",
    "for i in range(n_pairs):\n",
    "    model1 = init_model(seed=i)\n",
    "    model2 = init_model(seed=i+n_pairs)\n",
    "    \n",
    "    similarity = calculate_gradient_similarity(model1, model2)\n",
    "    similarities.append(similarity)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_similarity = np.mean(similarities)\n",
    "std_similarity = np.std(similarities)\n",
    "\n",
    "print(f\"Mean gradient similarity: {mean_similarity:.4f}\")\n",
    "print(f\"Standard deviation of gradient similarity: {std_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean gradient similarity: 0.0109\n",
      "Standard deviation of gradient similarity: 0.0563\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import the MNIST model from models.py\n",
    "from models import MNISTNet\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to initialize a model with a given seed\n",
    "def init_model(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    return MNISTNet().to(device)\n",
    "\n",
    "def gray_to_rgb(img):\n",
    "    return img.repeat(3, 1, 1)\n",
    "\n",
    "def load_mnist_data():\n",
    "    # Define data transform for MNIST data\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Lambda(gray_to_rgb), \n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    # Generate MNIST training data\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "        root='./data', \n",
    "        train=True,\n",
    "        download=True, \n",
    "        transform=transform,\n",
    "    )\n",
    "    \n",
    "    trainloader = DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "    \n",
    "    return next(iter(trainloader))\n",
    "\n",
    "# Function to calculate gradient similarity between two models\n",
    "def calculate_gradient_similarity(model1, model2, inputs, targets):\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    # Calculate gradients\n",
    "    grad1 = get_gradients_simple(inputs, targets, model1, criterion)\n",
    "    grad2 = get_gradients_simple(inputs, targets, model2, criterion)\n",
    "    \n",
    "    # Calculate similarity\n",
    "    similarity = np.dot(grad1, grad2) / (np.linalg.norm(grad1) * np.linalg.norm(grad2))\n",
    "    \n",
    "    return similarity\n",
    "\n",
    "def get_gradients_simple(X, y, model, criterion):\n",
    "    model.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, y).mean()  # Use mean reduction\n",
    "    loss.backward()\n",
    "    \n",
    "    grad_list = []\n",
    "    for name, p in model.named_parameters():\n",
    "        if p.grad is not None:\n",
    "            grad_list.append(p.grad.detach().cpu().numpy().flatten())\n",
    "    \n",
    "    return np.concatenate(grad_list)\n",
    "\n",
    "# Load MNIST data\n",
    "inputs, targets = load_mnist_data()\n",
    "inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "# Run the experiment\n",
    "n_pairs = 50\n",
    "similarities = []\n",
    "\n",
    "for i in range(n_pairs):\n",
    "    model1 = init_model(seed=i)\n",
    "    model2 = init_model(seed=i+n_pairs)\n",
    "    \n",
    "    similarity = calculate_gradient_similarity(model1, model2, inputs, targets)\n",
    "    similarities.append(similarity)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_similarity = np.mean(similarities)\n",
    "std_similarity = np.std(similarities)\n",
    "\n",
    "print(f\"Mean gradient similarity: {mean_similarity:.4f}\")\n",
    "print(f\"Standard deviation of gradient similarity: {std_similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking grad similarity with M1 and M2. This does it the exact same way as in the original code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 0, classes 0, 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n0/64ym_vyj0vg99kz4mpf9f6r80000gn/T/ipykernel_1759/2912845004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0mclasses_per_task\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mmax_data_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mtasks_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tasks_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_data_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_data_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses_per_task\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses_per_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Combine training data for tasks 1 through T-1 (omit the last task)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/continual-learning/train_task.py\u001b[0m in \u001b[0;36mmake_tasks_data\u001b[0;34m(trainset, testset, num_tasks, max_data_size, classes_per_task)\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;31m# Select classes for task t in training set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mfirst_two_classes_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mclasses_per_task\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmax_data_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                 \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0;31m# Create input and output data as pytoch tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2972\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2974\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2975\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2976\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   2890\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_MAPMODES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2892\u001b[0;31m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2893\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   2804\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2805\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2806\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from models import MNISTNet\n",
    "from train_task import make_tasks_data, get_gradients, combine_memory_sets\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gray_to_rgb(img):\n",
    "    return img.repeat(3, 1, 1)\n",
    "\n",
    "def load_mnist_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(gray_to_rgb),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "        root='./data',\n",
    "        train=True,\n",
    "        download=True,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    return trainset\n",
    "\n",
    "def init_model(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    return MNISTNet().to(device)\n",
    "\n",
    "def calculate_gradient_similarity(model_1, model_2, random_model, combined_train_x, combined_train_y, criterion):\n",
    "    # Evaluate model gradients on combined training set\n",
    "    model_1_full_grad = get_gradients(combined_train_x, combined_train_y, model_1, criterion)\n",
    "    model_2_full_grad = get_gradients(combined_train_x, combined_train_y, model_2, criterion)\n",
    "    random_model_grad = get_gradients(combined_train_x, combined_train_y, random_model, criterion)\n",
    "\n",
    "    # Compute gradient similarity for model 1 and 2 vs random model on full training sets\n",
    "    sim_1 = np.dot(model_1_full_grad, random_model_grad) / (np.linalg.norm(model_1_full_grad) * np.linalg.norm(random_model_grad))\n",
    "    sim_2 = np.dot(model_2_full_grad, random_model_grad) / (np.linalg.norm(model_2_full_grad) * np.linalg.norm(random_model_grad))\n",
    "\n",
    "    return sim_1, sim_2\n",
    "\n",
    "# Load MNIST data\n",
    "trainset = load_mnist_data()\n",
    "\n",
    "# Create tasks data\n",
    "num_tasks = 5\n",
    "classes_per_task = 2\n",
    "max_data_size = 6000\n",
    "tasks_data, _ = make_tasks_data(trainset, trainset, num_tasks=num_tasks, max_data_size=max_data_size, classes_per_task=classes_per_task)\n",
    "\n",
    "# Combine training data for tasks 1 through T-1 (omit the last task)\n",
    "combined_train_x, combined_train_y = combine_memory_sets(tasks_data, omit_task=1)\n",
    "combined_train_x, combined_train_y = combined_train_x.to(device), combined_train_y.to(device)\n",
    "\n",
    "# Load pre-trained M1 and M2 models\n",
    "model_1 = MNISTNet().to(device)\n",
    "model_2 = MNISTNet().to(device)\n",
    "model_1.load_state_dict(torch.load('ideal_models_M1.pth'))\n",
    "model_2.load_state_dict(torch.load('ideal_models_M2.pth'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "n_pairs = 1\n",
    "similarities_1 = []\n",
    "similarities_2 = []\n",
    "\n",
    "for i in range(n_pairs):\n",
    "    random_model = init_model(seed=i)\n",
    "    sim_1, sim_2 = calculate_gradient_similarity(model_1, model_2, random_model, combined_train_x, combined_train_y, criterion)\n",
    "    similarities_1.append(sim_1)\n",
    "    similarities_2.append(sim_2)\n",
    "\n",
    "# Calculate statistics\n",
    "mean_similarity_1 = np.mean(similarities_1)\n",
    "std_similarity_1 = np.std(similarities_1)\n",
    "mean_similarity_2 = np.mean(similarities_2)\n",
    "std_similarity_2 = np.std(similarities_2)\n",
    "\n",
    "print(f\"M1 - Mean gradient similarity: {mean_similarity_1:.4f}\")\n",
    "print(f\"M1 - Standard deviation of gradient similarity: {std_similarity_1:.4f}\")\n",
    "print(f\"M2 - Mean gradient similarity: {mean_similarity_2:.4f}\")\n",
    "print(f\"M2 - Standard deviation of gradient similarity: {std_similarity_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task 0, classes 0, 1\n",
      "task 1, classes 2, 3\n",
      "task 2, classes 4, 5\n",
      "task 3, classes 6, 7\n",
      "task 4, classes 8, 9\n",
      "here1\n",
      "here2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [24000, 2] at index 1 does not match the shape of the indexed tensor [24000, 3, 28, 28] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n0/64ym_vyj0vg99kz4mpf9f6r80000gn/T/ipykernel_1759/3860472490.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0mmemory_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomMemorySetManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"here2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mmemory_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_memory_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_train_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0msim_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_gradient_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/continual-learning/data.py\u001b[0m in \u001b[0;36mcreate_memory_set\u001b[0;34m(self, x, y, class_balanced)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m                     \u001b[0mx_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m                     \u001b[0my_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [24000, 2] at index 1 does not match the shape of the indexed tensor [24000, 3, 28, 28] at index 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from models import MNISTNet\n",
    "from train_task import get_gradients\n",
    "from data import RandomMemorySetManager\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gray_to_rgb(img):\n",
    "    return img.repeat(3, 1, 1)\n",
    "\n",
    "def load_mnist_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(gray_to_rgb),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    return trainset\n",
    "\n",
    "def random_sample_mnist(trainset, num_samples, exclude_class):\n",
    "    indices = []\n",
    "    labels = []\n",
    "    for i, (_, label) in enumerate(trainset):\n",
    "        if label != exclude_class:\n",
    "            indices.append(i)\n",
    "            labels.append(label)\n",
    "    \n",
    "    sample_indices = np.random.choice(indices, num_samples, replace=False)\n",
    "    sampled_data = torch.stack([trainset[i][0] for i in sample_indices])\n",
    "    sampled_labels = torch.tensor([labels[indices.index(i)] for i in sample_indices])\n",
    "    \n",
    "    return sampled_data, sampled_labels\n",
    "\n",
    "def calculate_gradient_similarity(model, memory_x, memory_y, full_x, full_y, criterion):\n",
    "    memory_grad = get_gradients(memory_x, memory_y, model, criterion)\n",
    "    full_grad = get_gradients(full_x, full_y, model, criterion)\n",
    "    \n",
    "    similarity = np.dot(memory_grad, full_grad) / (np.linalg.norm(memory_grad) * np.linalg.norm(full_grad))\n",
    "    return similarity\n",
    "\n",
    "# Load MNIST data\n",
    "trainset = load_mnist_data()\n",
    "\n",
    "# Randomly sample MNIST data, excluding the last class (9)\n",
    "num_samples = 50000  # Adjust this number as needed\n",
    "full_x, full_y = random_sample_mnist(trainset, num_samples, exclude_class=9)\n",
    "full_x, full_y = full_x.to(device), full_y.to(device)\n",
    "\n",
    "# Load pre-trained M1 and M2 models\n",
    "model_1 = MNISTNet().to(device)\n",
    "model_2 = MNISTNet().to(device)\n",
    "model_1.load_state_dict(torch.load('ideal_models_M1.pth'))\n",
    "model_2.load_state_dict(torch.load('ideal_models_M2.pth'))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "# Define p values\n",
    "p_values = [0.001]\n",
    "# p_values = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 0.9]\n",
    "\n",
    "\n",
    "similarities_1 = []\n",
    "similarities_2 = []\n",
    "\n",
    "for p in p_values:\n",
    "    memory_size = int(p * num_samples)\n",
    "    memory_manager = RandomMemorySetManager(p, random_seed=42)\n",
    "    memory_x, memory_y = memory_manager.create_memory_set(full_x, full_y)\n",
    "    \n",
    "    sim_1 = calculate_gradient_similarity(model_1, memory_x, memory_y, full_x, full_y, criterion)\n",
    "    sim_2 = calculate_gradient_similarity(model_2, memory_x, memory_y, full_x, full_y, criterion)\n",
    "    \n",
    "    similarities_1.append(sim_1)\n",
    "    similarities_2.append(sim_2)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(p_values, similarities_1, 'b-o', label='M1')\n",
    "plt.plot(p_values, similarities_2, 'r-o', label='M2')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('p value')\n",
    "plt.ylabel('Gradient Similarity')\n",
    "plt.title('Gradient Similarity vs p value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('gradient_similarity_vs_p.png')\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "for p, sim_1, sim_2 in zip(p_values, similarities_1, similarities_2):\n",
    "    print(f\"p = {p:.3f}: M1 similarity = {sim_1:.4f}, M2 similarity = {sim_2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

Using CUDA
Running experiment cifar10:
Results are stored in: cifar_first_experiment/cifar10
with hyperparameters {'p': 0.002, 'T': 5, 'learning_rate': 0.001, 'batch_size': 50, 'num_centroids': 4, 'model_training_epoch': 30, 'early_stopping_threshold': 100000, 'random_seed': 3, 'class_balanced': True, 'execute_early_stopping': False, 'exp_name': 'cifar10'}


Files already downloaded and verified
Files already downloaded and verified
task 0, classes 0, 1
task 1, classes 2, 3
task 2, classes 4, 5
task 3, classes 6, 7
task 4, classes 8, 9
Training models M1 and M2
Training model M1
epoch 1 train loss: 31002.529, val loss: 2720.981, train acc: 0.463, val acc: 0.566
diff inf
epoch 2 train loss: 22875.345, val loss: 2412.854, train acc: 0.614, val acc: 0.631
diff 308.1270348512958
epoch 3 train loss: 18606.746, val loss: 2185.177, train acc: 0.691, val acc: 0.667
diff 227.6772730844409
epoch 4 train loss: 14824.921, val loss: 2167.385, train acc: 0.753, val acc: 0.685
diff 17.792247041295468
epoch 5 train loss: 11318.973, val loss: 2455.976, train acc: 0.813, val acc: 0.684
diff 288.591558921029
epoch 6 train loss: 8105.658, val loss: 2697.816, train acc: 0.864, val acc: 0.679
diff 241.83932791709913
epoch 7 train loss: 5700.279, val loss: 3227.019, train acc: 0.908, val acc: 0.678
diff 529.2035721528491
epoch 8 train loss: 4366.836, val loss: 3653.312, train acc: 0.930, val acc: 0.674
diff 426.2925127452604
epoch 9 train loss: 3496.277, val loss: 3991.446, train acc: 0.944, val acc: 0.664
diff 338.1346911889277
epoch 10 train loss: 2909.297, val loss: 4316.509, train acc: 0.956, val acc: 0.671
diff 325.0624797754849
epoch 11 train loss: 2694.272, val loss: 5187.996, train acc: 0.959, val acc: 0.661
diff 871.4872146516209
epoch 12 train loss: 2943.737, val loss: 4843.392, train acc: 0.958, val acc: 0.670
diff 344.60445323521526
epoch 13 train loss: 2623.944, val loss: 5158.387, train acc: 0.961, val acc: 0.677
diff 314.9955628421403
epoch 14 train loss: 2422.579, val loss: 6386.916, train acc: 0.964, val acc: 0.658
diff 1228.5291211548338
epoch 15 train loss: 2453.944, val loss: 6045.860, train acc: 0.965, val acc: 0.659
diff 341.05616450816797
epoch 16 train loss: 2074.205, val loss: 6056.771, train acc: 0.971, val acc: 0.664
diff 10.910518202781532
epoch 17 train loss: 2349.534, val loss: 5944.472, train acc: 0.967, val acc: 0.657
diff 112.29808348405822
epoch 18 train loss: 2118.774, val loss: 6458.038, train acc: 0.971, val acc: 0.659
diff 513.5654580743403
epoch 19 train loss: 2361.346, val loss: 6415.514, train acc: 0.968, val acc: 0.668
diff 42.52372680713779
epoch 20 train loss: 2201.058, val loss: 6567.992, train acc: 0.971, val acc: 0.657
diff 152.47782292099873
epoch 21 train loss: 2015.230, val loss: 7074.017, train acc: 0.974, val acc: 0.660
diff 506.025026608002
epoch 22 train loss: 1768.801, val loss: 7149.779, train acc: 0.975, val acc: 0.664
diff 75.76227059670782
epoch 23 train loss: 1940.275, val loss: 7468.076, train acc: 0.975, val acc: 0.662
diff 318.2969407521614
epoch 24 train loss: 1899.211, val loss: 7188.351, train acc: 0.976, val acc: 0.666
diff 279.72502797993457
epoch 25 train loss: 2002.378, val loss: 7466.361, train acc: 0.976, val acc: 0.675
diff 278.01000814598046
epoch 26 train loss: 2047.407, val loss: 8243.722, train acc: 0.974, val acc: 0.654
diff 777.3605250202718
epoch 27 train loss: 1796.822, val loss: 8471.744, train acc: 0.978, val acc: 0.670
diff 228.02241466805208
epoch 28 train loss: 2179.632, val loss: 7660.519, train acc: 0.973, val acc: 0.666
diff 811.2256065044758
epoch 29 train loss: 1823.709, val loss: 7947.853, train acc: 0.978, val acc: 0.662
diff 287.3339771004612
epoch 30 train loss: 1947.762, val loss: 8299.396, train acc: 0.977, val acc: 0.666
diff 351.5434140582611
Training model M2
epoch 1 train loss: 40120.950, val loss: 3474.880, train acc: 0.451, val acc: 0.586
diff inf
epoch 2 train loss: 29370.131, val loss: 3157.285, train acc: 0.611, val acc: 0.630
diff 317.59534667264825
epoch 3 train loss: 23694.155, val loss: 2859.129, train acc: 0.688, val acc: 0.675
diff 298.15573509525757
epoch 4 train loss: 19308.158, val loss: 2779.345, train acc: 0.748, val acc: 0.687
diff 79.78393935186386
epoch 5 train loss: 15406.668, val loss: 2967.682, train acc: 0.798, val acc: 0.689
diff 188.3372239912328
epoch 6 train loss: 11566.548, val loss: 3359.823, train acc: 0.848, val acc: 0.682
diff 392.1406967223875
epoch 7 train loss: 8850.008, val loss: 3470.191, train acc: 0.884, val acc: 0.691
diff 110.36780576206684
epoch 8 train loss: 6716.700, val loss: 4050.084, train acc: 0.912, val acc: 0.688
diff 579.893843956429
epoch 9 train loss: 5642.892, val loss: 4936.474, train acc: 0.928, val acc: 0.664
diff 886.3892121147073
epoch 10 train loss: 4943.559, val loss: 5378.097, train acc: 0.938, val acc: 0.664
diff 441.6236011786523
epoch 11 train loss: 4533.624, val loss: 5627.595, train acc: 0.945, val acc: 0.681
diff 249.49817530213568
epoch 12 train loss: 4153.662, val loss: 5955.451, train acc: 0.950, val acc: 0.686
diff 327.8551608674452
epoch 13 train loss: 3956.153, val loss: 6027.056, train acc: 0.953, val acc: 0.672
diff 71.60510100902411
epoch 14 train loss: 3779.061, val loss: 6278.518, train acc: 0.955, val acc: 0.672
diff 251.462787444897
epoch 15 train loss: 3579.702, val loss: 6724.920, train acc: 0.958, val acc: 0.671
diff 446.4017729050638
epoch 16 train loss: 3467.055, val loss: 7630.807, train acc: 0.960, val acc: 0.667
diff 905.8868749857374
epoch 17 train loss: 3817.535, val loss: 7336.318, train acc: 0.958, val acc: 0.671
diff 294.48957984895605
epoch 18 train loss: 3545.110, val loss: 7213.290, train acc: 0.961, val acc: 0.667
diff 123.02785854127251
epoch 19 train loss: 3338.731, val loss: 7974.454, train acc: 0.963, val acc: 0.669
diff 761.1639711280422
epoch 20 train loss: 2944.427, val loss: 8689.172, train acc: 0.968, val acc: 0.665
diff 714.717903005886
epoch 21 train loss: 3220.554, val loss: 8161.230, train acc: 0.965, val acc: 0.664
diff 527.9416756845185
epoch 22 train loss: 3211.412, val loss: 8480.593, train acc: 0.966, val acc: 0.672
diff 319.3628234646276
epoch 23 train loss: 3279.030, val loss: 8735.448, train acc: 0.967, val acc: 0.673
diff 254.8550940924597
epoch 24 train loss: 2955.120, val loss: 9891.626, train acc: 0.969, val acc: 0.683
diff 1156.1783722904238
epoch 25 train loss: 3048.448, val loss: 8795.348, train acc: 0.969, val acc: 0.683
diff 1096.2777001655177
epoch 26 train loss: 3064.217, val loss: 9012.922, train acc: 0.970, val acc: 0.671
diff 217.57335999020324
epoch 27 train loss: 2826.544, val loss: 9550.545, train acc: 0.971, val acc: 0.672
diff 537.623487474817
epoch 28 train loss: 2869.203, val loss: 9176.886, train acc: 0.970, val acc: 0.669
diff 373.6595559545367
epoch 29 train loss: 3084.598, val loss: 9916.646, train acc: 0.970, val acc: 0.651
diff 739.7602066716681
epoch 30 train loss: 2767.271, val loss: 9361.729, train acc: 0.972, val acc: 0.678
diff 554.9169012883485
Selecting using KMeansMemorySetManager
Training model M3
epoch 1 train loss: 2727.818, val loss: 21995.571, train acc: 0.776, val acc: 0.180
diff inf
epoch 2 train loss: 1588.367, val loss: 18453.744, train acc: 0.894, val acc: 0.187
diff 3541.826920878426
epoch 3 train loss: 1254.381, val loss: 23198.069, train acc: 0.916, val acc: 0.188
diff 4744.32466833336
epoch 4 train loss: 1011.841, val loss: 19115.240, train acc: 0.936, val acc: 0.187
diff 4082.8284151312873
epoch 5 train loss: 838.969, val loss: 20599.557, train acc: 0.945, val acc: 0.186
diff 1484.3169851380153
epoch 6 train loss: 672.571, val loss: 21782.404, train acc: 0.955, val acc: 0.188
diff 1182.8465182363725
epoch 7 train loss: 557.195, val loss: 24440.469, train acc: 0.963, val acc: 0.188
diff 2658.065120559426
epoch 8 train loss: 432.518, val loss: 35223.871, train acc: 0.972, val acc: 0.192
diff 10783.402065713955
epoch 9 train loss: 364.448, val loss: 33835.788, train acc: 0.974, val acc: 0.189
diff 1388.0832251186293
epoch 10 train loss: 299.988, val loss: 33274.688, train acc: 0.982, val acc: 0.195
diff 561.0994378805844
epoch 11 train loss: 177.082, val loss: 34648.298, train acc: 0.990, val acc: 0.196
diff 1373.609782805972
epoch 12 train loss: 202.412, val loss: 42710.740, train acc: 0.987, val acc: 0.194
diff 8062.442080303947
epoch 13 train loss: 193.630, val loss: 43157.226, train acc: 0.987, val acc: 0.198
diff 446.4857958330467
epoch 14 train loss: 118.737, val loss: 53913.012, train acc: 0.992, val acc: 0.195
diff 10755.786309873816
epoch 15 train loss: 147.346, val loss: 46638.997, train acc: 0.990, val acc: 0.190
diff 7274.01551733962
epoch 16 train loss: 143.495, val loss: 43894.999, train acc: 0.993, val acc: 0.201
diff 2743.9981253737787
epoch 17 train loss: 147.153, val loss: 52981.839, train acc: 0.991, val acc: 0.196
diff 9086.840070113903
epoch 18 train loss: 51.887, val loss: 60210.784, train acc: 0.997, val acc: 0.191
diff 7228.945529987381
epoch 19 train loss: 204.043, val loss: 50164.176, train acc: 0.989, val acc: 0.188
diff 10046.607801631508
epoch 20 train loss: 192.476, val loss: 52490.457, train acc: 0.988, val acc: 0.196
diff 2326.2806100086673
epoch 21 train loss: 101.773, val loss: 55925.884, train acc: 0.995, val acc: 0.193
diff 3435.4270865047947
epoch 22 train loss: 51.023, val loss: 58438.903, train acc: 0.997, val acc: 0.193
diff 2513.0189316910037
epoch 23 train loss: 63.615, val loss: 58084.987, train acc: 0.996, val acc: 0.191
diff 353.91612119630736
epoch 24 train loss: 126.264, val loss: 53142.172, train acc: 0.994, val acc: 0.192
diff 4942.8150834783955
epoch 25 train loss: 185.120, val loss: 66031.320, train acc: 0.988, val acc: 0.188
diff 12889.148649877592
epoch 26 train loss: 90.304, val loss: 64773.930, train acc: 0.993, val acc: 0.191
diff 1257.3900507313956
epoch 27 train loss: 2.173, val loss: 70481.698, train acc: 1.000, val acc: 0.192
diff 5707.7674705560785
epoch 28 train loss: 0.434, val loss: 73692.385, train acc: 1.000, val acc: 0.191
diff 3210.6869397125847
epoch 29 train loss: 0.169, val loss: 75845.732, train acc: 1.000, val acc: 0.191
diff 2153.34726568684
epoch 30 train loss: 0.107, val loss: 78129.764, train acc: 1.000, val acc: 0.191
diff 2284.0316465380165
Done.

Using CUDA
Running experiment cifar10:
Results are stored in: cifar_first_experiment/cifar10
with hyperparameters {'p': 0.9, 'T': 5, 'learning_rate': 0.001, 'batch_size': 50, 'num_centroids': 4, 'model_training_epoch': 30, 'early_stopping_threshold': 100000, 'random_seed': 1, 'class_balanced': True, 'execute_early_stopping': False, 'exp_name': 'cifar10'}


Files already downloaded and verified
Files already downloaded and verified
task 0, classes 0, 1
task 1, classes 2, 3
task 2, classes 4, 5
task 3, classes 6, 7
task 4, classes 8, 9
Training models M1 and M2
Training model M1
epoch 1 train loss: 31091.149, val loss: 2626.018, train acc: 0.457, val acc: 0.585
diff inf
epoch 2 train loss: 23014.607, val loss: 2352.566, train acc: 0.610, val acc: 0.637
diff 273.4519584214099
epoch 3 train loss: 19068.669, val loss: 2161.841, train acc: 0.679, val acc: 0.673
diff 190.72486343209857
epoch 4 train loss: 15768.952, val loss: 2178.674, train acc: 0.736, val acc: 0.683
diff 16.832709984752455
epoch 5 train loss: 12459.155, val loss: 2217.475, train acc: 0.793, val acc: 0.693
diff 38.800543215072594
epoch 6 train loss: 9496.537, val loss: 2505.938, train acc: 0.842, val acc: 0.680
diff 288.4631758644482
epoch 7 train loss: 6982.997, val loss: 3068.886, train acc: 0.886, val acc: 0.680
diff 562.9481600640756
epoch 8 train loss: 5237.232, val loss: 3195.477, train acc: 0.912, val acc: 0.680
diff 126.59121608332316
epoch 9 train loss: 4192.268, val loss: 3879.026, train acc: 0.932, val acc: 0.671
diff 683.5483150776995
epoch 10 train loss: 3501.212, val loss: 4007.690, train acc: 0.944, val acc: 0.677
diff 128.6639696705829
epoch 11 train loss: 3289.367, val loss: 4379.718, train acc: 0.950, val acc: 0.674
diff 372.028311240163
epoch 12 train loss: 2986.251, val loss: 4726.092, train acc: 0.954, val acc: 0.682
diff 346.37431439574084
epoch 13 train loss: 2762.258, val loss: 5215.307, train acc: 0.957, val acc: 0.682
diff 489.2146583135245
epoch 14 train loss: 2955.418, val loss: 5076.149, train acc: 0.956, val acc: 0.686
diff 139.15769648202058
epoch 15 train loss: 2386.284, val loss: 5821.455, train acc: 0.964, val acc: 0.674
diff 745.3060000022615
epoch 16 train loss: 2520.999, val loss: 6034.943, train acc: 0.963, val acc: 0.663
diff 213.4878248232917
epoch 17 train loss: 2625.228, val loss: 5880.816, train acc: 0.964, val acc: 0.677
diff 154.12683784161436
epoch 18 train loss: 2251.361, val loss: 6453.461, train acc: 0.968, val acc: 0.659
diff 572.6444706253696
epoch 19 train loss: 2496.550, val loss: 6133.523, train acc: 0.966, val acc: 0.679
diff 319.9375223250345
epoch 20 train loss: 2278.363, val loss: 6072.025, train acc: 0.969, val acc: 0.671
diff 61.498172687003716
epoch 21 train loss: 2187.083, val loss: 6324.490, train acc: 0.970, val acc: 0.686
diff 252.46472377682585
epoch 22 train loss: 2387.272, val loss: 6766.532, train acc: 0.969, val acc: 0.672
diff 442.0425300368779
epoch 23 train loss: 2268.346, val loss: 6965.952, train acc: 0.971, val acc: 0.683
diff 199.42001311045624
epoch 24 train loss: 2223.471, val loss: 7361.376, train acc: 0.971, val acc: 0.661
diff 395.42388042302264
epoch 25 train loss: 2410.347, val loss: 6417.121, train acc: 0.970, val acc: 0.674
diff 944.2552561381926
epoch 26 train loss: 2017.022, val loss: 7022.582, train acc: 0.975, val acc: 0.660
diff 605.4616820632227
epoch 27 train loss: 2120.132, val loss: 7311.666, train acc: 0.972, val acc: 0.664
diff 289.0836671472407
epoch 28 train loss: 1825.327, val loss: 7522.613, train acc: 0.976, val acc: 0.673
diff 210.94656491533988
epoch 29 train loss: 2011.014, val loss: 8351.132, train acc: 0.975, val acc: 0.663
diff 828.5197408058011
epoch 30 train loss: 2102.329, val loss: 7980.764, train acc: 0.975, val acc: 0.665
diff 370.3680465583402
Training model M2
epoch 1 train loss: 39357.771, val loss: 3460.552, train acc: 0.468, val acc: 0.582
diff inf
epoch 2 train loss: 28667.853, val loss: 3099.495, train acc: 0.619, val acc: 0.623
diff 361.05665201125294
epoch 3 train loss: 23376.136, val loss: 2776.377, train acc: 0.690, val acc: 0.686
diff 323.1176225834424
epoch 4 train loss: 18873.616, val loss: 2743.861, train acc: 0.751, val acc: 0.691
diff 32.51633819069275
epoch 5 train loss: 14899.212, val loss: 3065.863, train acc: 0.805, val acc: 0.683
diff 322.0018474927574
epoch 6 train loss: 11252.107, val loss: 3144.520, train acc: 0.851, val acc: 0.687
diff 78.65680363377805
epoch 7 train loss: 8444.365, val loss: 3731.419, train acc: 0.890, val acc: 0.685
diff 586.8990498089829
epoch 8 train loss: 6694.037, val loss: 4138.361, train acc: 0.914, val acc: 0.677
diff 406.9425637057584
epoch 9 train loss: 5477.543, val loss: 4852.257, train acc: 0.929, val acc: 0.660
diff 713.8961844927726
epoch 10 train loss: 4998.563, val loss: 4837.239, train acc: 0.939, val acc: 0.678
diff 15.018689291895498
epoch 11 train loss: 4327.752, val loss: 5624.018, train acc: 0.946, val acc: 0.683
diff 786.7790450155153
epoch 12 train loss: 4308.930, val loss: 5686.311, train acc: 0.948, val acc: 0.674
diff 62.29286565165694
epoch 13 train loss: 3820.247, val loss: 6254.693, train acc: 0.955, val acc: 0.684
diff 568.3823884431495
epoch 14 train loss: 3686.697, val loss: 6703.706, train acc: 0.956, val acc: 0.674
diff 449.01278452799943
epoch 15 train loss: 3520.494, val loss: 7086.325, train acc: 0.958, val acc: 0.671
diff 382.6189227244813
epoch 16 train loss: 3692.756, val loss: 7182.711, train acc: 0.958, val acc: 0.667
diff 96.38637886774814
epoch 17 train loss: 3301.001, val loss: 7555.543, train acc: 0.964, val acc: 0.679
diff 372.8324104579651
epoch 18 train loss: 3464.585, val loss: 7679.513, train acc: 0.962, val acc: 0.678
diff 123.9699834569983
epoch 19 train loss: 3057.940, val loss: 8047.994, train acc: 0.966, val acc: 0.673
diff 368.4805196029356
epoch 20 train loss: 3354.676, val loss: 8344.321, train acc: 0.963, val acc: 0.677
diff 296.3269931810328
epoch 21 train loss: 3118.039, val loss: 9030.448, train acc: 0.967, val acc: 0.659
diff 686.1267276534782
epoch 22 train loss: 3399.498, val loss: 7856.713, train acc: 0.965, val acc: 0.675
diff 1173.73442820302
epoch 23 train loss: 3222.440, val loss: 8659.228, train acc: 0.966, val acc: 0.662
diff 802.5147162691328
epoch 24 train loss: 3255.521, val loss: 8794.313, train acc: 0.967, val acc: 0.677
diff 135.08548700340725
epoch 25 train loss: 2997.936, val loss: 8931.429, train acc: 0.970, val acc: 0.671
diff 137.11599158132958
epoch 26 train loss: 3064.291, val loss: 9115.057, train acc: 0.969, val acc: 0.664
diff 183.6272900273034
epoch 27 train loss: 3251.653, val loss: 9427.875, train acc: 0.968, val acc: 0.675
diff 312.8177802529299
epoch 28 train loss: 3322.553, val loss: 9816.174, train acc: 0.968, val acc: 0.653
diff 388.29956443458104
epoch 29 train loss: 2627.565, val loss: 9736.735, train acc: 0.974, val acc: 0.677
diff 79.43870564051213
epoch 30 train loss: 2989.436, val loss: 9522.719, train acc: 0.971, val acc: 0.673
diff 214.01639105506365
Selecting using KMeansMemorySetManager
Training model M3
epoch 1 train loss: 39511.445, val loss: 3461.371, train acc: 0.459, val acc: 0.587
diff inf
epoch 2 train loss: 28844.823, val loss: 2813.937, train acc: 0.614, val acc: 0.661
diff 647.4347922850457
epoch 3 train loss: 23477.627, val loss: 2427.814, train acc: 0.688, val acc: 0.719
diff 386.1226653967087
epoch 4 train loss: 19480.543, val loss: 2068.880, train acc: 0.741, val acc: 0.766
diff 358.9340105672968
epoch 5 train loss: 15449.588, val loss: 1921.456, train acc: 0.793, val acc: 0.802
diff 147.423529739204
epoch 6 train loss: 12051.541, val loss: 1650.319, train acc: 0.840, val acc: 0.844
diff 271.13712411667643
epoch 7 train loss: 8667.163, val loss: 1506.396, train acc: 0.886, val acc: 0.876
diff 143.9233662406266
epoch 8 train loss: 6897.279, val loss: 1818.737, train acc: 0.910, val acc: 0.873
diff 312.34054458143873
epoch 9 train loss: 5558.628, val loss: 1753.986, train acc: 0.929, val acc: 0.889
diff 64.75058168983014
epoch 10 train loss: 5037.087, val loss: 1924.909, train acc: 0.937, val acc: 0.895
diff 170.9233178809734
epoch 11 train loss: 4353.034, val loss: 1914.568, train acc: 0.945, val acc: 0.899
diff 10.341532537621788
epoch 12 train loss: 4272.544, val loss: 2359.551, train acc: 0.948, val acc: 0.891
diff 444.9831321876354
epoch 13 train loss: 3754.847, val loss: 1988.855, train acc: 0.955, val acc: 0.903
diff 370.6953761980021
epoch 14 train loss: 3736.715, val loss: 2530.279, train acc: 0.956, val acc: 0.891
diff 541.4238918428227
epoch 15 train loss: 3444.220, val loss: 2347.890, train acc: 0.961, val acc: 0.897
diff 182.38916673047243
epoch 16 train loss: 3611.447, val loss: 2158.622, train acc: 0.957, val acc: 0.905
diff 189.268055427523
epoch 17 train loss: 3348.159, val loss: 2472.886, train acc: 0.963, val acc: 0.889
diff 314.2642734629012
epoch 18 train loss: 3217.874, val loss: 2565.262, train acc: 0.964, val acc: 0.895
diff 92.37543148435543
epoch 19 train loss: 3390.380, val loss: 2506.757, train acc: 0.962, val acc: 0.903
diff 58.50461359216615
epoch 20 train loss: 3128.212, val loss: 2570.795, train acc: 0.965, val acc: 0.902
diff 64.03761547425393
epoch 21 train loss: 3224.135, val loss: 3037.360, train acc: 0.964, val acc: 0.893
diff 466.56503450170203
epoch 22 train loss: 3159.412, val loss: 2776.385, train acc: 0.968, val acc: 0.900
diff 260.974718751429
epoch 23 train loss: 3073.199, val loss: 2610.590, train acc: 0.969, val acc: 0.898
diff 165.79538497667045
epoch 24 train loss: 3156.657, val loss: 2814.308, train acc: 0.967, val acc: 0.903
diff 203.71820682870384
epoch 25 train loss: 3047.579, val loss: 2793.947, train acc: 0.969, val acc: 0.900
diff 20.36147431851623
epoch 26 train loss: 3161.865, val loss: 2450.497, train acc: 0.969, val acc: 0.911
diff 343.449341734487
epoch 27 train loss: 3138.118, val loss: 3521.031, train acc: 0.969, val acc: 0.887
diff 1070.533970716182
epoch 28 train loss: 2875.405, val loss: 3247.045, train acc: 0.971, val acc: 0.896
diff 273.98657777586914
epoch 29 train loss: 3159.953, val loss: 3702.098, train acc: 0.969, val acc: 0.887
diff 455.05323673596604
epoch 30 train loss: 2941.648, val loss: 3374.068, train acc: 0.972, val acc: 0.901
diff 328.0300746805301
Done.

Using CUDA
Running experiment cifar10:
Results are stored in: cifar_first_experiment/cifar10
with hyperparameters {'p': 0.9, 'T': 5, 'learning_rate': 0.001, 'batch_size': 50, 'num_centroids': 4, 'model_training_epoch': 30, 'early_stopping_threshold': 100000, 'random_seed': 4, 'class_balanced': True, 'execute_early_stopping': False, 'exp_name': 'cifar10'}


Files already downloaded and verified
Files already downloaded and verified
task 0, classes 0, 1
task 1, classes 2, 3
task 2, classes 4, 5
task 3, classes 6, 7
task 4, classes 8, 9
Training models M1 and M2
Training model M1
epoch 1 train loss: 31513.551, val loss: 2814.641, train acc: 0.453, val acc: 0.560
diff inf
epoch 2 train loss: 23120.487, val loss: 2322.676, train acc: 0.610, val acc: 0.642
diff 491.9648940972079
epoch 3 train loss: 18665.670, val loss: 2199.630, train acc: 0.688, val acc: 0.668
diff 123.04574563456572
epoch 4 train loss: 15020.577, val loss: 2154.095, train acc: 0.749, val acc: 0.683
diff 45.5347701687092
epoch 5 train loss: 11613.415, val loss: 2217.707, train acc: 0.808, val acc: 0.702
diff 63.61213955614403
epoch 6 train loss: 8203.218, val loss: 2531.719, train acc: 0.865, val acc: 0.691
diff 314.0111709795342
epoch 7 train loss: 5673.170, val loss: 3026.112, train acc: 0.907, val acc: 0.692
diff 494.3929835549402
epoch 8 train loss: 4417.314, val loss: 3748.576, train acc: 0.929, val acc: 0.672
diff 722.464107617237
epoch 9 train loss: 3547.556, val loss: 3926.199, train acc: 0.944, val acc: 0.685
diff 177.6229630286075
epoch 10 train loss: 3298.230, val loss: 4336.236, train acc: 0.949, val acc: 0.661
diff 410.03763715669356
epoch 11 train loss: 2951.444, val loss: 4416.404, train acc: 0.954, val acc: 0.674
diff 80.16827282819304
epoch 12 train loss: 2614.494, val loss: 4710.485, train acc: 0.962, val acc: 0.663
diff 294.0807057108532
epoch 13 train loss: 2723.102, val loss: 5051.430, train acc: 0.960, val acc: 0.682
diff 340.9446370908954
epoch 14 train loss: 2500.508, val loss: 5541.403, train acc: 0.964, val acc: 0.684
diff 489.9727470313637
epoch 15 train loss: 2124.267, val loss: 5362.362, train acc: 0.967, val acc: 0.680
diff 179.040244283221
epoch 16 train loss: 2554.844, val loss: 5282.215, train acc: 0.965, val acc: 0.680
diff 80.14750658884441
epoch 17 train loss: 2252.232, val loss: 5757.758, train acc: 0.968, val acc: 0.665
diff 475.5435090636902
epoch 18 train loss: 1986.830, val loss: 7099.814, train acc: 0.972, val acc: 0.674
diff 1342.0560672642996
epoch 19 train loss: 2287.301, val loss: 6419.294, train acc: 0.969, val acc: 0.677
diff 680.5203905654271
epoch 20 train loss: 1999.537, val loss: 7403.026, train acc: 0.973, val acc: 0.667
diff 983.7320845049335
epoch 21 train loss: 2048.803, val loss: 6790.054, train acc: 0.973, val acc: 0.675
diff 612.9722970729799
epoch 22 train loss: 2117.006, val loss: 6972.502, train acc: 0.973, val acc: 0.668
diff 182.44820278274983
epoch 23 train loss: 1908.440, val loss: 7662.651, train acc: 0.975, val acc: 0.660
diff 690.1492408117674
epoch 24 train loss: 2080.903, val loss: 7677.866, train acc: 0.973, val acc: 0.660
diff 15.214621638237077
epoch 25 train loss: 2419.216, val loss: 8590.031, train acc: 0.972, val acc: 0.671
diff 912.1653155483882
epoch 26 train loss: 1765.639, val loss: 8301.547, train acc: 0.977, val acc: 0.659
diff 288.4841174670619
epoch 27 train loss: 2174.733, val loss: 8446.016, train acc: 0.973, val acc: 0.665
diff 144.46886743522737
epoch 28 train loss: 1695.715, val loss: 8085.904, train acc: 0.978, val acc: 0.675
diff 360.1118401658232
epoch 29 train loss: 1929.027, val loss: 8423.168, train acc: 0.977, val acc: 0.666
diff 337.2636145731476
epoch 30 train loss: 1974.411, val loss: 8964.852, train acc: 0.976, val acc: 0.672
diff 541.684627648212
Training model M2
epoch 1 train loss: 40052.048, val loss: 3423.240, train acc: 0.452, val acc: 0.577
diff inf
epoch 2 train loss: 28754.415, val loss: 3333.002, train acc: 0.620, val acc: 0.594
diff 90.23820765033497
epoch 3 train loss: 22809.486, val loss: 2756.611, train acc: 0.703, val acc: 0.677
diff 576.390895018832
epoch 4 train loss: 18170.425, val loss: 2924.932, train acc: 0.760, val acc: 0.672
diff 168.32071761018324
epoch 5 train loss: 14031.143, val loss: 3074.500, train acc: 0.816, val acc: 0.679
diff 149.56805554541415
epoch 6 train loss: 10565.960, val loss: 3484.980, train acc: 0.861, val acc: 0.674
diff 410.48017747463246
epoch 7 train loss: 7564.935, val loss: 3906.449, train acc: 0.901, val acc: 0.690
diff 421.46949603647454
epoch 8 train loss: 6122.765, val loss: 4342.668, train acc: 0.921, val acc: 0.694
diff 436.21826266465405
epoch 9 train loss: 5313.796, val loss: 4816.915, train acc: 0.935, val acc: 0.686
diff 474.2472833935726
epoch 10 train loss: 4613.999, val loss: 5091.338, train acc: 0.944, val acc: 0.683
diff 274.4234873876667
epoch 11 train loss: 4389.507, val loss: 5620.934, train acc: 0.946, val acc: 0.686
diff 529.5952375992711
epoch 12 train loss: 4120.234, val loss: 5632.688, train acc: 0.952, val acc: 0.693
diff 11.753831245728179
epoch 13 train loss: 3988.907, val loss: 6156.429, train acc: 0.953, val acc: 0.694
diff 523.7413365738566
epoch 14 train loss: 3565.918, val loss: 6379.714, train acc: 0.957, val acc: 0.696
diff 223.2852639072089
epoch 15 train loss: 3705.008, val loss: 6781.231, train acc: 0.959, val acc: 0.685
diff 401.51722885776144
epoch 16 train loss: 3366.936, val loss: 6963.078, train acc: 0.963, val acc: 0.679
diff 181.8467712854963
epoch 17 train loss: 3565.691, val loss: 6670.998, train acc: 0.961, val acc: 0.677
diff 292.08034219170713
epoch 18 train loss: 3499.692, val loss: 6911.179, train acc: 0.962, val acc: 0.672
diff 240.1816643125012
epoch 19 train loss: 3028.213, val loss: 7727.213, train acc: 0.966, val acc: 0.683
diff 816.0338487920108
epoch 20 train loss: 3321.398, val loss: 8411.910, train acc: 0.964, val acc: 0.669
diff 684.6966836031979
epoch 21 train loss: 3210.048, val loss: 7917.262, train acc: 0.966, val acc: 0.681
diff 494.6479116202627
epoch 22 train loss: 3079.509, val loss: 7855.909, train acc: 0.967, val acc: 0.690
diff 61.35298968762436
epoch 23 train loss: 3143.888, val loss: 8477.317, train acc: 0.968, val acc: 0.677
diff 621.4080882837479
epoch 24 train loss: 2943.218, val loss: 8088.587, train acc: 0.970, val acc: 0.682
diff 388.73034205128897
epoch 25 train loss: 3446.238, val loss: 8265.023, train acc: 0.965, val acc: 0.688
diff 176.43613533069765
epoch 26 train loss: 2737.438, val loss: 8729.585, train acc: 0.973, val acc: 0.677
diff 464.56188203362944
epoch 27 train loss: 3254.787, val loss: 9177.144, train acc: 0.967, val acc: 0.669
diff 447.55886889929025
epoch 28 train loss: 2893.931, val loss: 9772.711, train acc: 0.972, val acc: 0.681
diff 595.5676684415484
epoch 29 train loss: 2933.148, val loss: 9984.270, train acc: 0.972, val acc: 0.681
diff 211.5582398487204
epoch 30 train loss: 3047.376, val loss: 9675.478, train acc: 0.971, val acc: 0.673
diff 308.7917816335921
Selecting using KMeansMemorySetManager
Training model M3
epoch 1 train loss: 38886.146, val loss: 3408.055, train acc: 0.466, val acc: 0.585
diff inf
epoch 2 train loss: 28145.661, val loss: 2714.948, train acc: 0.628, val acc: 0.681
diff 693.1069849807031
epoch 3 train loss: 22737.011, val loss: 2338.337, train acc: 0.696, val acc: 0.736
diff 376.61092055135805
epoch 4 train loss: 18112.119, val loss: 1990.272, train acc: 0.762, val acc: 0.778
diff 348.0649293574129
epoch 5 train loss: 14058.693, val loss: 1643.858, train acc: 0.814, val acc: 0.835
diff 346.4143241129523
epoch 6 train loss: 10419.319, val loss: 1628.853, train acc: 0.864, val acc: 0.862
diff 15.004262430143626
epoch 7 train loss: 7678.168, val loss: 1630.647, train acc: 0.898, val acc: 0.866
diff 1.7940688880444213
epoch 8 train loss: 6387.300, val loss: 1714.057, train acc: 0.917, val acc: 0.877
diff 83.40969721487772
epoch 9 train loss: 5177.507, val loss: 2159.553, train acc: 0.934, val acc: 0.880
diff 445.49586171810233
epoch 10 train loss: 4898.969, val loss: 1922.288, train acc: 0.940, val acc: 0.901
diff 237.2645608622022
epoch 11 train loss: 4321.743, val loss: 1862.619, train acc: 0.947, val acc: 0.890
diff 59.66957185201204
epoch 12 train loss: 3959.751, val loss: 2003.714, train acc: 0.952, val acc: 0.903
diff 141.095278464177
epoch 13 train loss: 3741.831, val loss: 1998.477, train acc: 0.955, val acc: 0.900
diff 5.236620117816301
epoch 14 train loss: 3617.760, val loss: 2337.068, train acc: 0.956, val acc: 0.891
diff 338.5908904590701
epoch 15 train loss: 3629.778, val loss: 2518.052, train acc: 0.959, val acc: 0.886
diff 180.9837810109043
epoch 16 train loss: 3549.351, val loss: 2189.746, train acc: 0.960, val acc: 0.907
diff 328.3062153071596
epoch 17 train loss: 3525.616, val loss: 2428.384, train acc: 0.961, val acc: 0.904
diff 238.63835454753143
epoch 18 train loss: 3010.063, val loss: 2846.076, train acc: 0.966, val acc: 0.891
diff 417.6918019612317
epoch 19 train loss: 3488.188, val loss: 2760.155, train acc: 0.962, val acc: 0.899
diff 85.92150954166755
epoch 20 train loss: 3334.747, val loss: 2910.735, train acc: 0.964, val acc: 0.893
diff 150.5803164101585
epoch 21 train loss: 3220.109, val loss: 2662.289, train acc: 0.966, val acc: 0.896
diff 248.44593987515827
epoch 22 train loss: 3061.825, val loss: 2509.552, train acc: 0.967, val acc: 0.904
diff 152.73739678478387
epoch 23 train loss: 3189.788, val loss: 2694.816, train acc: 0.967, val acc: 0.908
diff 185.264272027051
epoch 24 train loss: 2881.434, val loss: 3082.803, train acc: 0.971, val acc: 0.899
diff 387.98738609643897
epoch 25 train loss: 3096.479, val loss: 2945.492, train acc: 0.970, val acc: 0.900
diff 137.31097675944784
epoch 26 train loss: 3019.938, val loss: 3186.257, train acc: 0.969, val acc: 0.887
diff 240.764415458043
epoch 27 train loss: 3030.864, val loss: 3264.212, train acc: 0.971, val acc: 0.906
diff 77.95500375784923
epoch 28 train loss: 3100.934, val loss: 3634.036, train acc: 0.970, val acc: 0.901
diff 369.82398515607883
epoch 29 train loss: 3098.131, val loss: 3726.324, train acc: 0.970, val acc: 0.895
diff 92.287947138861
epoch 30 train loss: 2862.721, val loss: 3029.040, train acc: 0.973, val acc: 0.905
diff 697.2837706420023
Done.

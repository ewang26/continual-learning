Using CUDA
Running experiment mnist:
Results are stored in: output_500_new/mnist
with hyperparameters {'p': 0.1, 'T': 5, 'learning_rate': 0.001, 'batch_size': 50, 'num_centroids': 4, 'model_training_epoch': 30, 'early_stopping_threshold': 0.1, 'random_seed': 0, 'class_balanced': True, 'exp_name': 'mnist'}


task 0, classes 0, 1
task 1, classes 2, 3
task 2, classes 4, 5
task 3, classes 6, 7
task 4, classes 8, 9
Training models M1 and M2
Training model M1
epoch 1 train loss: 2460.479, val loss: 123.867, train acc: 0.966, val acc: 0.985
diff inf
epoch 2 train loss: 751.943, val loss: 144.683, train acc: 0.989, val acc: 0.984
diff 20.81630358188832
epoch 3 train loss: 511.368, val loss: 79.854, train acc: 0.993, val acc: 0.992
diff 64.82912344685786
epoch 4 train loss: 335.965, val loss: 98.634, train acc: 0.996, val acc: 0.990
diff 18.779958518947694
epoch 5 train loss: 270.576, val loss: 176.650, train acc: 0.996, val acc: 0.984
diff 78.01574727013013
epoch 6 train loss: 248.534, val loss: 102.028, train acc: 0.997, val acc: 0.992
diff 74.62210952492491
epoch 7 train loss: 186.974, val loss: 217.516, train acc: 0.998, val acc: 0.983
diff 115.48791454690333
epoch 8 train loss: 187.995, val loss: 104.880, train acc: 0.998, val acc: 0.993
diff 112.63618134672296
epoch 9 train loss: 105.653, val loss: 207.513, train acc: 0.999, val acc: 0.990
diff 102.63371336295637
epoch 10 train loss: 238.589, val loss: 192.720, train acc: 0.997, val acc: 0.990
diff 14.793724677862798
epoch 11 train loss: 109.074, val loss: 192.341, train acc: 0.998, val acc: 0.988
diff 0.3787585096973203
epoch 12 train loss: 146.593, val loss: 235.447, train acc: 0.998, val acc: 0.988
diff 43.10576542588595
epoch 13 train loss: 169.000, val loss: 230.418, train acc: 0.998, val acc: 0.989
diff 5.028207785257678
epoch 14 train loss: 132.325, val loss: 149.656, train acc: 0.999, val acc: 0.993
diff 80.76241687755589
epoch 15 train loss: 176.239, val loss: 253.028, train acc: 0.998, val acc: 0.991
diff 103.37248617215067
epoch 16 train loss: 224.191, val loss: 220.455, train acc: 0.998, val acc: 0.988
diff 32.57297524898732
epoch 17 train loss: 83.563, val loss: 245.031, train acc: 0.999, val acc: 0.993
diff 24.57601439234449
epoch 18 train loss: 173.449, val loss: 429.874, train acc: 0.998, val acc: 0.983
diff 184.84254384309915
epoch 19 train loss: 131.923, val loss: 239.847, train acc: 0.999, val acc: 0.991
diff 190.02667134334376
epoch 20 train loss: 66.802, val loss: 293.522, train acc: 0.999, val acc: 0.991
diff 53.67450119463143
epoch 21 train loss: 145.397, val loss: 300.446, train acc: 0.999, val acc: 0.993
diff 6.924622797660504
epoch 22 train loss: 65.943, val loss: 219.236, train acc: 0.999, val acc: 0.993
diff 81.21065837135902
epoch 23 train loss: 154.884, val loss: 485.105, train acc: 0.999, val acc: 0.990
diff 265.86929289090074
epoch 24 train loss: 119.563, val loss: 347.153, train acc: 0.999, val acc: 0.993
diff 137.95195167568323
epoch 25 train loss: 53.499, val loss: 463.770, train acc: 1.000, val acc: 0.993
diff 116.61706645128987
epoch 26 train loss: 118.556, val loss: 543.743, train acc: 0.999, val acc: 0.990
diff 79.9731455244945
epoch 27 train loss: 184.565, val loss: 465.612, train acc: 0.999, val acc: 0.993
diff 78.13142717839287

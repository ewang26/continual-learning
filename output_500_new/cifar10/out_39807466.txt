Using CUDA
Running experiment cifar10:
Results are stored in: output_500_new/cifar10
with hyperparameters {'p': 0.01, 'T': 5, 'learning_rate': 0.001, 'batch_size': 50, 'num_centroids': 4, 'model_training_epoch': 30, 'early_stopping_threshold': 0.1, 'random_seed': 5, 'class_balanced': True, 'exp_name': 'cifar10'}


Files already downloaded and verified
Files already downloaded and verified
task 0, classes 0, 1
task 1, classes 2, 3
task 2, classes 4, 5
task 3, classes 6, 7
task 4, classes 8, 9
Training models M1 and M2
Training model M1
epoch 1 train loss: 30949.548, val loss: 2772.024, train acc: 0.464, val acc: 0.551
diff inf
epoch 2 train loss: 22586.413, val loss: 2361.212, train acc: 0.621, val acc: 0.647
diff 410.8118117834392
epoch 3 train loss: 18143.281, val loss: 2178.878, train acc: 0.694, val acc: 0.673
diff 182.33433993746485
epoch 4 train loss: 14383.345, val loss: 2182.693, train acc: 0.763, val acc: 0.673
diff 3.8147746691179236
epoch 5 train loss: 10812.895, val loss: 2456.641, train acc: 0.821, val acc: 0.677
diff 273.9478467930435
epoch 6 train loss: 7487.716, val loss: 2919.578, train acc: 0.874, val acc: 0.672
diff 462.9378304829088
epoch 7 train loss: 5434.910, val loss: 3435.810, train acc: 0.914, val acc: 0.675
diff 516.2317101848439
epoch 8 train loss: 4109.735, val loss: 3917.163, train acc: 0.935, val acc: 0.666
diff 481.3529273708068
epoch 9 train loss: 3422.160, val loss: 4580.792, train acc: 0.946, val acc: 0.658
diff 663.6294891839825
epoch 10 train loss: 3282.699, val loss: 5074.850, train acc: 0.950, val acc: 0.668
diff 494.05801080461697
epoch 11 train loss: 2727.244, val loss: 5051.430, train acc: 0.958, val acc: 0.661
diff 23.42065143345826
epoch 12 train loss: 2858.190, val loss: 5226.646, train acc: 0.959, val acc: 0.666
diff 175.21634912382433
epoch 13 train loss: 2417.774, val loss: 5743.350, train acc: 0.967, val acc: 0.657
diff 516.7042874078261
epoch 14 train loss: 2638.131, val loss: 5614.215, train acc: 0.963, val acc: 0.675
diff 129.13529242840832
epoch 15 train loss: 2329.677, val loss: 6124.955, train acc: 0.967, val acc: 0.667
diff 510.7401654342675
epoch 16 train loss: 2289.076, val loss: 5847.566, train acc: 0.969, val acc: 0.675
diff 277.3893254949717
epoch 17 train loss: 2455.498, val loss: 5546.805, train acc: 0.966, val acc: 0.693
diff 300.76146035048623
epoch 18 train loss: 2065.717, val loss: 6821.709, train acc: 0.971, val acc: 0.662
diff 1274.9039655113284
epoch 19 train loss: 2196.733, val loss: 7010.048, train acc: 0.970, val acc: 0.660
diff 188.33978477777146
epoch 20 train loss: 2047.447, val loss: 6900.363, train acc: 0.973, val acc: 0.666
diff 109.68547914501687
epoch 21 train loss: 2097.475, val loss: 7671.399, train acc: 0.972, val acc: 0.663
diff 771.0365617963416
epoch 22 train loss: 2159.028, val loss: 7135.796, train acc: 0.974, val acc: 0.645
diff 535.6037345357017
epoch 23 train loss: 1803.778, val loss: 7056.487, train acc: 0.976, val acc: 0.669
diff 79.30886676157843
epoch 24 train loss: 1958.808, val loss: 7366.441, train acc: 0.975, val acc: 0.654
diff 309.9543227374379
epoch 25 train loss: 1984.218, val loss: 7867.474, train acc: 0.975, val acc: 0.657
diff 501.0324332155242
epoch 26 train loss: 2084.493, val loss: 8272.462, train acc: 0.975, val acc: 0.666
diff 404.98865413474323

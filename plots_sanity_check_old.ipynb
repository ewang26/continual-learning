{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c8f693b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import os\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import glob\n",
    "from run import OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03954bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar_test_aug_3\n"
     ]
    }
   ],
   "source": [
    "print(OUTPUT_DIR)\n",
    "# OUTPUT_DIR = 'cifar_first_experiment'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a42a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RandomMemorySetManager</th>\n",
       "      <th>execution_time</th>\n",
       "      <th>p</th>\n",
       "      <th>T</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_centroids</th>\n",
       "      <th>model_training_epoch</th>\n",
       "      <th>early_stopping_threshold</th>\n",
       "      <th>random_seed</th>\n",
       "      <th>class_balanced</th>\n",
       "      <th>max_data_size</th>\n",
       "      <th>execute_early_stopping</th>\n",
       "      <th>managers</th>\n",
       "      <th>exp_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'model performances': {'M2': 0.144, 'M3': 0.0...</td>\n",
       "      <td>00:02:01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>RandomMemorySetManager</td>\n",
       "      <td>cifar10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'model performances': {'M2': 0.144, 'M3': 0.0...</td>\n",
       "      <td>00:02:09</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cifar10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'model performances': {'M2': 0.144, 'M3': 0.0...</td>\n",
       "      <td>00:02:04</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>False</td>\n",
       "      <td>LambdaMemorySetManager</td>\n",
       "      <td>cifar10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              RandomMemorySetManager execution_time    p  T  \\\n",
       "0  {'model performances': {'M2': 0.144, 'M3': 0.0...       00:02:01  0.1  5   \n",
       "1  {'model performances': {'M2': 0.144, 'M3': 0.0...       00:02:09  0.1  5   \n",
       "2  {'model performances': {'M2': 0.144, 'M3': 0.0...       00:02:04  0.1  5   \n",
       "\n",
       "   learning_rate  batch_size  num_centroids  model_training_epoch  \\\n",
       "0            0.1          10              4                     1   \n",
       "1            0.1          10              4                     1   \n",
       "2            0.1          10              4                     1   \n",
       "\n",
       "   early_stopping_threshold  random_seed  class_balanced  max_data_size  \\\n",
       "0                       5.0            0            True            100   \n",
       "1                       5.0            0            True            100   \n",
       "2                       5.0            0            True            100   \n",
       "\n",
       "   execute_early_stopping                managers exp_name  \n",
       "0                   False  RandomMemorySetManager  cifar10  \n",
       "1                   False                     NaN  cifar10  \n",
       "2                   False  LambdaMemorySetManager  cifar10  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_results():\n",
    "    results = []\n",
    "    for fname in glob.glob(f'{OUTPUT_DIR}/*/*'):\n",
    "        # print('Reading from {}'.format(fname))\n",
    "        if os.path.splitext(fname)[1] == '.pickle':  # Only try to unpickle .pickle files\n",
    "            try:\n",
    "                with open(fname, 'rb') as f:\n",
    "                    result = pickle.load(f)\n",
    "                    results.append(result)\n",
    "            except (pickle.UnpicklingError, EOFError, AttributeError) as e:\n",
    "                print(f\"Error unpickling {fname}: {str(e)}\")\n",
    "        # else:\n",
    "        #     print(f\"Skipping non-pickle file: {fname}\")\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "results = load_results()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da17c7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_no=4\n",
    "def get_metric(df, p_val, method, metric):\n",
    "    #returns the average and std for a given metric \n",
    "    p_value_df = df[df[\"p\"] == p_val].reset_index(drop=True)\n",
    "    method_metrics = p_value_df[method]\n",
    "    metric_values = []\n",
    "    for i in range(0, len(method_metrics)):         \n",
    "         if metric == \"Accuracy\":\n",
    "            metric_values.append(method_metrics[i]['M3 per task performance'][task_no])\n",
    "\n",
    "         if metric == \"M2 similarity\":\n",
    "             metric_values.append(method_metrics[i]['gradient similarities']['M2'])\n",
    "\n",
    "         if metric == \"M1 similarity\":\n",
    "             metric_values.append(method_metrics[i]['gradient similarities']['M1'])\n",
    "             \n",
    "    return metric_values\n",
    "\n",
    "def get_metric_stats(metric_values):\n",
    "    return np.average(metric_values), np.std(metric_values)\n",
    "\n",
    "\n",
    "# metrics = get_metric(results, 0.01, \"RandomMemorySetManager memory selection\", \"Accuracy\")\n",
    "# get_metric_stats(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572753ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def aggregate_results(results):\n",
    "    methods = [\n",
    "        # \"RandomMemorySetManager memory selection\",\n",
    "        \"KMeansMemorySetManager\",\n",
    "        # \"LambdaMemorySetManager memory selection\",\n",
    "        # \"iCaRL memory selection (icarl)\",\n",
    "        # \"iCaRL memory selection (replay)\"\n",
    "    ]\n",
    "    # method_names = [\"Random\", \"KMeans\", \"Lambda\", 'icarl', 'icarl replay']\n",
    "    # metrics = [\"Accuracy\", \"M1 similarity\", \"M2 similarity\"]\n",
    "    method_names = [\"KMeans\"]\n",
    "    metrics = [\"Accuracy\", \"M1 similarity\", \"M2 similarity\"]\n",
    "    unique_p_values = results['p'].unique()\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    for p in unique_p_values:\n",
    "        for method, method_name in zip(methods, method_names):\n",
    "            filtered_data = results[(results['p'] == p) & (results[method].notna())]\n",
    "            \n",
    "            if not filtered_data.empty:\n",
    "                row_data = {\n",
    "                    'p': p,\n",
    "                    'method': method_name,\n",
    "                    'samples': len(filtered_data)\n",
    "                }\n",
    "                \n",
    "                for metric in metrics:\n",
    "                    metric_values = get_metric(filtered_data, p, method, metric)\n",
    "                    avg, std = get_metric_stats(metric_values)\n",
    "                    \n",
    "                    if metric == \"Accuracy\":\n",
    "                        row_data['M3_task4_avg'] = avg\n",
    "                        row_data['M3_task4_std'] = std\n",
    "                    elif metric == \"M1 similarity\":\n",
    "                        row_data['M1_grad_sim_avg'] = avg\n",
    "                        row_data['M1_grad_sim_std'] = std\n",
    "                    elif metric == \"M2 similarity\":\n",
    "                        row_data['M2_grad_sim_avg'] = avg\n",
    "                        row_data['M2_grad_sim_std'] = std\n",
    "                \n",
    "                processed_data.append(row_data)\n",
    "    \n",
    "    processed_df = pd.DataFrame(processed_data)\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results = aggregate_results(results)\n",
    "aggregated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005f9d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_results(results):\n",
    "    methods = [\n",
    "    #     \"RandomMemorySetManager memory selection\",\n",
    "        \"KMeansMemorySetManager\",\n",
    "    #     \"LambdaMemorySetManager memory selection\",\n",
    "    #     \"iCaRL memory selection (icarl)\",\n",
    "    #     \"iCaRL memory selection (replay)\"\n",
    "    ]\n",
    "    # method_names = [\"Random\", \"KMeans\", \"Lambda\", 'icarl', 'icarl replay']\n",
    "    metrics = [\"Accuracy\", \"M1 similarity\", \"M2 similarity\"]\n",
    "    method_names=[\"k-means\"]\n",
    "    unique_p_values = results['p'].unique()\n",
    "    \n",
    "    processed_data = []\n",
    "    \n",
    "    for p in unique_p_values:\n",
    "        for method, method_name in zip(methods, method_names):\n",
    "            filtered_data = results[(results['p'] == p) & (results[method].notna())]\n",
    "            \n",
    "            if not filtered_data.empty:\n",
    "                row_data = {\n",
    "                    'p': p,\n",
    "                    'method': method_name,\n",
    "                    'samples': len(filtered_data)\n",
    "                }\n",
    "                \n",
    "                for metric in metrics:\n",
    "                    metric_values = get_metric(filtered_data, p, method, metric)\n",
    "                    avg, std = get_metric_stats(metric_values)\n",
    "                    \n",
    "                    if metric == \"Accuracy\":\n",
    "                        row_data['M3_task4_avg'] = avg\n",
    "                        row_data['M3_task4_std'] = std\n",
    "                        row_data['M3_task4_array'] = metric_values\n",
    "                    elif metric == \"M1 similarity\":\n",
    "                        row_data['M1_grad_sim_avg'] = avg\n",
    "                        row_data['M1_grad_sim_std'] = std\n",
    "                        row_data['M1_grad_sim_array'] = metric_values\n",
    "                    elif metric == \"M2 similarity\":\n",
    "                        row_data['M2_grad_sim_avg'] = avg\n",
    "                        row_data['M2_grad_sim_std'] = std\n",
    "                        row_data['M2_grad_sim_array'] = metric_values\n",
    "                \n",
    "                processed_data.append(row_data)\n",
    "    \n",
    "    processed_df = pd.DataFrame(processed_data)\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daf552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results = aggregate_results(results)\n",
    "aggregated_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def plot_all_metrics(aggregated_results, dataset_name):\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    result_dir = f'gradient_similarity/{dataset_name}/plots'\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    \n",
    "    methods = aggregated_results['method'].unique()\n",
    "    p_vals = sorted(aggregated_results['p'].unique())\n",
    "    \n",
    "    num_methods = len(methods)\n",
    "    fig, axs = plt.subplots(num_methods, 3, figsize=(24, 4*num_methods))\n",
    "    \n",
    "    metrics = ['M3_task4', 'M1_grad_sim', 'M2_grad_sim']\n",
    "    titles = [f'M3 Task {task_no} Average Accuracy', 'M1 Gradient Similarity', 'M2 Gradient Similarity']\n",
    "    \n",
    "    for method_index, method in enumerate(methods):\n",
    "        method_data = aggregated_results[aggregated_results['method'] == method]\n",
    "        \n",
    "        for col, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "            if num_methods == 1:\n",
    "                ax = axs[col]\n",
    "            else:\n",
    "                ax = axs[method_index, col]\n",
    "            \n",
    "            avg_values = [method_data[method_data['p'] == p][f'{metric}_avg'].values[0] for p in p_vals]\n",
    "            std_values = [method_data[method_data['p'] == p][f'{metric}_std'].values[0] for p in p_vals]\n",
    "            \n",
    "            ax.plot(p_vals, avg_values, marker='o', label=method)\n",
    "            ax.fill_between(p_vals, \n",
    "                            np.array(avg_values) - np.array(std_values), \n",
    "                            np.array(avg_values) + np.array(std_values), \n",
    "                            alpha=0.2)\n",
    "            \n",
    "            ax.set_xscale('log')\n",
    "            ax.set_title(f'{title} for {method}', fontsize=10)\n",
    "            ax.legend(fontsize=8)\n",
    "            \n",
    "            ax.set_xticks(p_vals)\n",
    "            ax.set_xticklabels([f'{p:.2f}' for p in p_vals], rotation=45, ha='right')\n",
    "            \n",
    "            if method_index == num_methods - 1:  # Last row\n",
    "                ax.set_xlabel('p values')\n",
    "            \n",
    "            if col == 0:  # First column\n",
    "                ax.set_ylabel(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{result_dir}/all_metrics_vs_p_subplots.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_all_metrics(aggregated_results, 'dataset_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def plot_all_metrics(aggregated_results, dataset_name):\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    result_dir = f'gradient_similarity/{dataset_name}/plots'\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    \n",
    "    methods = aggregated_results['method'].unique()\n",
    "    p_vals = sorted(aggregated_results['p'].unique())\n",
    "    \n",
    "    num_methods = len(methods)\n",
    "    fig, axs = plt.subplots(num_methods, 3, figsize=(24, 4*num_methods))\n",
    "    \n",
    "    metrics = ['M3_task4', 'M1_grad_sim', 'M2_grad_sim']\n",
    "    titles = [f'M3 Task {task_no} Accuracy', 'M1 Gradient Similarity', 'M2 Gradient Similarity']\n",
    "    \n",
    "    for method_index, method in enumerate(methods):\n",
    "        method_data = aggregated_results[aggregated_results['method'] == method]\n",
    "        \n",
    "        for col, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "            if num_methods == 1:\n",
    "                ax = axs[col]\n",
    "            else:\n",
    "                ax = axs[method_index, col]\n",
    "            \n",
    "            # Get the number of samples\n",
    "            num_samples = len(method_data[method_data['p'] == p_vals[0]][f'{metric}_array'].values[0])\n",
    "            \n",
    "            for sample_index in range(num_samples):\n",
    "                sample_values = [method_data[method_data['p'] == p][f'{metric}_array'].values[0][sample_index] for p in p_vals]\n",
    "                ax.plot(p_vals, sample_values, alpha=0.3, linewidth=1)\n",
    "            \n",
    "            # Calculate and plot the mean\n",
    "            mean_values = [np.mean(method_data[method_data['p'] == p][f'{metric}_array'].values[0]) for p in p_vals]\n",
    "            ax.plot(p_vals, mean_values, color='red', linewidth=2, label='Mean')\n",
    "            \n",
    "            ax.set_xscale('log')\n",
    "            ax.set_title(f'{title} for {method}', fontsize=10)\n",
    "            ax.legend(fontsize=8)\n",
    "            \n",
    "            ax.set_xticks(p_vals)\n",
    "            ax.set_xticklabels([f'{p:.2f}' for p in p_vals], rotation=45, ha='right')\n",
    "            \n",
    "            if method_index == num_methods - 1:  # Last row\n",
    "                ax.set_xlabel('p values')\n",
    "            \n",
    "            if col == 0:  # First column\n",
    "                ax.set_ylabel(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{result_dir}/all_metrics_vs_p_subplots_sample_lines.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_all_metrics(aggregated_results, 'dataset_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af77100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def plot_all_metrics_normalize_axis(aggregated_results, dataset_name):\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    result_dir = f'gradient_similarity/{dataset_name}/plots'\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    \n",
    "    methods = aggregated_results['method'].unique()\n",
    "    p_vals = sorted(aggregated_results['p'].unique())\n",
    "    \n",
    "    num_methods = len(methods)\n",
    "    fig, axs = plt.subplots(num_methods, 3, figsize=(24, 4*num_methods))\n",
    "    \n",
    "    metrics = ['M3_task4', 'M1_grad_sim', 'M2_grad_sim']\n",
    "    titles = [f'M3 Task {task_no} Average', 'M1 Gradient Similarity', 'M2 Gradient Similarity']\n",
    "    \n",
    "    # Find global min and max for each metric, including std\n",
    "    y_limits = {}\n",
    "    for metric in metrics:\n",
    "        min_val = (aggregated_results[f'{metric}_avg'] - aggregated_results[f'{metric}_std']).min()\n",
    "        max_val = (aggregated_results[f'{metric}_avg'] + aggregated_results[f'{metric}_std']).max()\n",
    "        y_range = max_val - min_val\n",
    "        y_limits[metric] = (min_val - 0.1 * y_range, max_val + 0.1 * y_range)\n",
    "    \n",
    "    for method_index, method in enumerate(methods):\n",
    "        method_data = aggregated_results[aggregated_results['method'] == method]\n",
    "        \n",
    "        for col, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "            if num_methods == 1:\n",
    "                ax = axs[col]\n",
    "            else:\n",
    "                ax = axs[method_index, col]\n",
    "            \n",
    "            avg_values = [method_data[method_data['p'] == p][f'{metric}_avg'].values[0] for p in p_vals]\n",
    "            std_values = [method_data[method_data['p'] == p][f'{metric}_std'].values[0] for p in p_vals]\n",
    "            \n",
    "            ax.plot(p_vals, avg_values, marker='o', label=method)\n",
    "            ax.fill_between(p_vals, \n",
    "                            np.array(avg_values) - np.array(std_values), \n",
    "                            np.array(avg_values) + np.array(std_values), \n",
    "                            alpha=0.2)\n",
    "            \n",
    "            ax.set_xscale('log')\n",
    "            ax.set_title(f'{title} for {method}', fontsize=10)\n",
    "            ax.legend(fontsize=8)\n",
    "            \n",
    "            ax.set_xticks(p_vals)\n",
    "            ax.set_xticklabels([f'{p:.2f}' for p in p_vals], rotation=45, ha='right')\n",
    "            \n",
    "            # Set y-axis limits to be the same for all methods within each metric\n",
    "            ax.set_ylim(y_limits[metric])\n",
    "            \n",
    "            if method_index == num_methods - 1:  # Last row\n",
    "                ax.set_xlabel('p values')\n",
    "            \n",
    "            if col == 0:  # First column\n",
    "                ax.set_ylabel(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{result_dir}/all_metrics_vs_p_subplots.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_all_metrics_normalize_axis(aggregated_results, 'dataset_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07d9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def plot_all_metrics(aggregated_results, dataset_name):\n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    result_dir = f'gradient_similarity/{dataset_name}/plots'\n",
    "    if not os.path.exists(result_dir):\n",
    "        os.makedirs(result_dir)\n",
    "    \n",
    "    methods = aggregated_results['method'].unique()\n",
    "    p_vals = sorted(aggregated_results['p'].unique())\n",
    "    \n",
    "    num_methods = len(methods)\n",
    "    fig, axs = plt.subplots(num_methods, 3, figsize=(24, 4*num_methods))\n",
    "    \n",
    "    metrics = ['M3_task4', 'M1_grad_sim', 'M2_grad_sim']\n",
    "    titles = [f'M3 Task {task_no} Accuracy', 'M1 Gradient Similarity', 'M2 Gradient Similarity']\n",
    "    \n",
    "    for method_index, method in enumerate(methods):\n",
    "        method_data = aggregated_results[aggregated_results['method'] == method]\n",
    "        \n",
    "        for col, (metric, title) in enumerate(zip(metrics, titles)):\n",
    "            if num_methods == 1:\n",
    "                ax = axs[col]\n",
    "            else:\n",
    "                ax = axs[method_index, col]\n",
    "            \n",
    "            # Get the number of samples\n",
    "            num_samples = len(method_data[method_data['p'] == p_vals[0]][f'{metric}_array'].values[0])\n",
    "            \n",
    "            for sample_index in range(num_samples):\n",
    "                sample_values = [method_data[method_data['p'] == p][f'{metric}_array'].values[0][sample_index] for p in p_vals]\n",
    "                ax.plot(p_vals, sample_values, alpha=0.3, linewidth=1)\n",
    "            \n",
    "            # Calculate and plot the mean\n",
    "            mean_values = [np.mean(method_data[method_data['p'] == p][f'{metric}_array'].values[0]) for p in p_vals]\n",
    "            ax.plot(p_vals, mean_values, color='red', linewidth=2, label='Mean')\n",
    "            \n",
    "            ax.set_xscale('log')\n",
    "            ax.set_title(f'{title} for {method}', fontsize=10)\n",
    "            ax.legend(fontsize=8)\n",
    "            \n",
    "            ax.set_xticks(p_vals)\n",
    "            ax.set_xticklabels([f'{p:.2f}' for p in p_vals], rotation=45, ha='right')\n",
    "            \n",
    "            if method_index == num_methods - 1:  # Last row\n",
    "                ax.set_xlabel('p values')\n",
    "            \n",
    "            if col == 0:  # First column\n",
    "                ax.set_ylabel(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{result_dir}/all_metrics_vs_p_subplots_sample_lines.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_all_metrics(aggregated_results, 'dataset_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d48dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_dist(metric_name,\n",
    "                   p_vals, \n",
    "                   dataset_name, \n",
    "                   grad_type,\n",
    "                   method_arr,\n",
    "                   method_names,\n",
    "                   downstream_acc_type,\n",
    "                   variance_type,\n",
    "                   together = False):\n",
    "    \n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    \n",
    "    result_dir = f'gradient_similarity/{dataset_name}/plots'\n",
    "    if not os.path.exists(result_dir): os.mkdir(result_dir)\n",
    "    \n",
    "    if together: # just create 1 subplot for everything\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    \n",
    "    num_methods = len(method_arr)\n",
    "    if not together:\n",
    "        fig, axs = plt.subplots(num_methods, 2, figsize=(20, 4*num_methods))  # Adjust subplot size as needed\n",
    "\n",
    "    for method_index, method in enumerate(method_arr):\n",
    "        #if not together:\n",
    "        #    fig, axs = plt.subplots(1, 2, figsize=(20, 4))  # Adjust subplot size as needed\n",
    "        \n",
    "        if (num_methods == 1) or together:  # Check if there are multiple metrics to avoid indexing error on axs\n",
    "            ax = axs\n",
    "        else:\n",
    "            ax = axs[method_index]\n",
    "        \n",
    "        method_name = method_names[method_index]\n",
    "\n",
    "        # load accuracies\n",
    "        acc_block = np.load(f'gradient_similarity/{dataset_name}/{method}/acc_block.npy')\n",
    "        # acc_block is (p_val x num_tasks) of downstream acc performance\n",
    "        if downstream_acc_type == 'last':\n",
    "            acc_arr = acc_block[:, -1]\n",
    "        elif downstream_acc_type == 'avg':\n",
    "            acc_arr = np.mean(acc_block, axis = -1)\n",
    "        else:\n",
    "            print('invalid downstream acc type')\n",
    "            assert False\n",
    "\n",
    "        # load gradients (num_p, num_runs, num_ideal_models, num_tasks-1, num_grad_files)\n",
    "        grad_block = np.load(f'gradient_similarity/{dataset_name}/{method}/{metric_name}/{grad_type}_gradient_comp.npy')\n",
    "\n",
    "        avg_over_grad_files = np.mean(grad_block, axis = -1)\n",
    "        avg_over_tasks = np.mean(avg_over_grad_files, axis = -1)\n",
    "\n",
    "        if variance_type == 'dataset':\n",
    "            avg_grad_block = avg_over_tasks[:,:,0]\n",
    "            # avg over runs\n",
    "            grad_dists = np.mean(avg_grad_block, axis = -1)\n",
    "            grad_stds = np.std(avg_grad_block, axis = -1)\n",
    "        elif variance_type == 'weight':\n",
    "            avg_grad_block = avg_over_tasks[:,0,:]\n",
    "            # avg over ideal models\n",
    "            grad_dists = np.mean(avg_grad_block, axis = -1)\n",
    "            grad_stds = np.std(avg_grad_block, axis = -1)\n",
    "        elif variance_type == 'all':\n",
    "            grad_dists = np.mean(np.mean(avg_over_tasks, axis = -1), axis = -1)\n",
    "            grad_stds = (np.std(avg_over_tasks[:,:,0], axis = -1)**2 + np.std(avg_over_tasks[:,0,:], axis = -1)**2)**(1/2)\n",
    "        \n",
    "        ax[1].set_xlim(0, 1)\n",
    "        ax[0].plot(p_vals, grad_dists, marker='o', label = method_name)\n",
    "        if not together:\n",
    "            ax[0].fill_between(p_vals, grad_dists - grad_stds, grad_dists + grad_stds, alpha=0.2)\n",
    "        #ax[0].set_xlabel('p values')\n",
    "        #ax[0].set_ylabel('Gradient distance')\n",
    "        ax[0].set_xscale('log')\n",
    "        if not together:\n",
    "            ax[0].set_title(f'{metric_name} for {method_name}')\n",
    "\n",
    "        ax[1].plot(acc_arr, grad_dists, marker='o', label = method_name)\n",
    "        if not together:\n",
    "            ax[1].fill_between(acc_arr, grad_dists - grad_stds, grad_dists + grad_stds, alpha=0.2)\n",
    "        #ax[1].set_xlabel('Downstream accuracy')\n",
    "        #ax[1].set_ylabel('Gradient distance')\n",
    "        if not together:\n",
    "            ax[1].set_title(f'{metric_name} for {method_name}')\n",
    "            \n",
    "        if method_index == len(method_arr) -1: # plot for last row\n",
    "            ax[0].set_xlabel('p values')\n",
    "            ax[1].set_xlabel('Downstream accuracy')\n",
    "        elif method_index == 0:\n",
    "            ax[0].set_ylabel('Gradient distance')\n",
    "            \n",
    "            \n",
    "    \n",
    "    # save_path = f'{save_path}/task_{task}/{fig_name_prefix}'\n",
    "    # os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # if plot_acc: # plotting against accuracy\n",
    "    #     img_name = f'{save_path}/{dataset_name}_acc.png'\n",
    "    # else: # plotting against p_values\n",
    "    #     img_name = f'{save_path}/{dataset_name}_p.png'\n",
    "    # plt.savefig(img_name)\n",
    "    if not together:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{result_dir}/mem_indiv.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        axs[0].legend()\n",
    "        axs[1].legend()\n",
    "        \n",
    "        axs[0].set_title(f'{metric_name} vs P-value')\n",
    "        axs[1].set_title(f'{metric_name} vs Downstream Accuracy')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{result_dir}/mem_comp.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03406f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = 'Cosine Similarity'\n",
    "p_values = [0.001, 0.002, 0.005]\n",
    "dataset_name = 'mnist_split'\n",
    "grad_type = 'past'\n",
    "method_arr = ['class_balanced']\n",
    "method_names = [\"Class-Balanced Reservoir Sampling\"]\n",
    "downstream_acc_type = 'last' # can be last or avg\n",
    "variance_type = 'all' # can be dataset, weight, or all\n",
    "together = True\n",
    "\n",
    "plot_grad_dist(metric_name = metric_name,\n",
    "               p_vals = p_values, \n",
    "               dataset_name = dataset_name, \n",
    "               grad_type = grad_type,\n",
    "               method_arr = method_arr,\n",
    "               method_names = method_names,\n",
    "               downstream_acc_type = downstream_acc_type,\n",
    "               variance_type = variance_type,\n",
    "               together = together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51bf973",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = 'Cosine Similarity'\n",
    "p_values = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\n",
    "dataset_name = 'mnist_split'\n",
    "grad_type = 'past'\n",
    "method_arr = ['random', 'class_balanced', 'GSS', 'lambda', 'kmeans']\n",
    "method_names = ['Random', \"Class-Balanced Reservoir Sampling\", \"Gradient-based Sample Selection\", \"Lambda\", 'Kmeans']\n",
    "downstream_acc_type = 'last' # can be last or avg\n",
    "variance_type = 'all' # can be dataset, weight, or all\n",
    "together = True\n",
    "\n",
    "plot_grad_dist(metric_name = metric_name,\n",
    "               p_vals = p_values, \n",
    "               dataset_name = dataset_name, \n",
    "               grad_type = grad_type,\n",
    "               method_arr = method_arr,\n",
    "               method_names = method_names,\n",
    "               downstream_acc_type = downstream_acc_type,\n",
    "               variance_type = variance_type,\n",
    "               together = together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d871540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = 'Cosine Similarity'\n",
    "p_values = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\n",
    "dataset_name = 'mnist_split'\n",
    "grad_type = 'past'\n",
    "method_arr = ['random', 'class_balanced', 'GSS', 'lambda', 'kmeans']\n",
    "method_names = ['Random', \"Class-Balanced Reservoir Sampling\", \"Gradient-based Sample Selection\", \"Lambda\", \"K-means\"]\n",
    "downstream_acc_type = 'last' # can be last or avg\n",
    "variance_type = 'all' # can be dataset, weight, or all\n",
    "together = False\n",
    "\n",
    "plot_grad_dist(metric_name = metric_name,\n",
    "               p_vals = p_values, \n",
    "               dataset_name = dataset_name, \n",
    "               grad_type = grad_type,\n",
    "               method_arr = method_arr,\n",
    "               method_names = method_names,\n",
    "               downstream_acc_type = downstream_acc_type,\n",
    "               variance_type = variance_type,\n",
    "               together = together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501eabfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatterplot_grad_dist(metric_name,\n",
    "                   p_vals, \n",
    "                   dataset_name, \n",
    "                   grad_type,\n",
    "                   method_arr,\n",
    "                   method_names,\n",
    "                   downstream_acc_type,\n",
    "                   variance_type,\n",
    "                   p_indices):\n",
    "    \n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    \n",
    "    result_dir = f'gradient_similarity/{dataset_name}/plots'\n",
    "    if not os.path.exists(result_dir): os.mkdir(result_dir)\n",
    "        \n",
    "    fig, axs = plt.subplots(len(p_indices), 1, figsize=(20, 10*len(p_indices)))\n",
    "    \n",
    "    #plt.figure(figsize = (20, 10))\n",
    "    \n",
    "    num_methods = len(method_arr)\n",
    "    \n",
    "    for i, p_index in enumerate(p_indices):\n",
    "        \n",
    "        if (len(p_indices) == 1):\n",
    "            ax = axs\n",
    "        else:\n",
    "            ax = axs[i]\n",
    "        \n",
    "    \n",
    "        for method_index, method in enumerate(method_arr):\n",
    "\n",
    "            method_name = method_names[method_index]\n",
    "\n",
    "            acc_block = np.load(f'gradient_similarity/{dataset_name}/{method}/acc_block.npy')\n",
    "            # acc_block is (p_val x num_tasks) of downstream acc performance\n",
    "            if downstream_acc_type == 'last':\n",
    "                acc_arr = acc_block[:, -1]\n",
    "            elif downstream_acc_type == 'avg':\n",
    "                acc_arr = np.mean(acc_block, axis = -1)\n",
    "            else:\n",
    "                print('invalid downstream acc type')\n",
    "                assert False\n",
    "\n",
    "            # load gradients (num_p, num_runs, num_ideal_models, num_tasks-1, num_grad_files)\n",
    "            grad_block = np.load(f'gradient_similarity/{dataset_name}/{method}/{metric_name}/{grad_type}_gradient_comp.npy')\n",
    "\n",
    "            avg_over_grad_files = np.mean(grad_block, axis = -1)\n",
    "            avg_over_tasks = np.mean(avg_over_grad_files, axis = -1)\n",
    "\n",
    "            if variance_type == 'dataset':\n",
    "                avg_grad_block = avg_over_tasks[:,:,0]\n",
    "                # avg over runs\n",
    "                grad_dists = np.mean(avg_grad_block, axis = -1)\n",
    "                grad_stds = np.std(avg_grad_block, axis = -1)\n",
    "            elif variance_type == 'weight':\n",
    "                avg_grad_block = avg_over_tasks[:,0,:]\n",
    "                # avg over ideal models\n",
    "                grad_dists = np.mean(avg_grad_block, axis = -1)\n",
    "                grad_stds = np.std(avg_grad_block, axis = -1)\n",
    "            elif variance_type == 'all':\n",
    "                grad_dists = np.mean(np.mean(avg_over_tasks, axis = -1), axis = -1)\n",
    "                grad_stds = (np.std(avg_over_tasks[:,:,0], axis = -1)**2 + np.std(avg_over_tasks[:,0,:], axis = -1)**2)**(1/2)\n",
    "\n",
    "            grad_pt = grad_dists[p_index]\n",
    "            grad_var = grad_stds[p_index]\n",
    "            acc = acc_arr[p_index]\n",
    "            ax.scatter(acc, grad_pt, label = method_name)\n",
    "            ax.errorbar(acc, grad_pt, yerr = grad_var) \n",
    "            ax.set_title(f'Gradient Similarity vs Downstream Accuracy Scatterplot for p = {p_vals[p_index]}')\n",
    "            ax.set_xlim(0.2, 1)\n",
    "            ax.set_ylim(0, 1.1)\n",
    "            if i == len(p_indices)-1:\n",
    "                ax.set_xlabel('Downstream Accuracy')\n",
    "            ax.set_ylabel('Gradient Similarity')\n",
    "            ax.legend()\n",
    "            \n",
    "    #plt.xlim(0.2, 1)\n",
    "    #plt.ylim(0, 1.1)\n",
    "    #plt.suptitle(f'Gradient Similarity vs Downstream Accuracy for Memory Selection Methods')\n",
    "    #plt.ylabel('Gradient Similarity')\n",
    "    #plt.xlabel('Downstream Accuracy')\n",
    "    #plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{result_dir}/scatterplot_indiv.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d333d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = 'Cosine Similarity'\n",
    "p_values = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\n",
    "dataset_name = 'mnist_split'\n",
    "grad_type = 'past'\n",
    "method_arr = ['random', 'class_balanced', 'GSS', 'lambda', 'kmeans']\n",
    "method_names = ['Random', \"Class-Balanced Reservoir Sampling\", \"Gradient-based Sample Selection\", \"Lambda\", \"K-means\"]\n",
    "downstream_acc_type = 'last' # can be last or avg\n",
    "variance_type = 'all' # can be dataset, weight, or all\n",
    "p_indices = [0, 3, 6, 8]\n",
    "\n",
    "scatterplot_grad_dist(metric_name = metric_name,\n",
    "               p_vals = p_values, \n",
    "               dataset_name = dataset_name, \n",
    "               grad_type = grad_type,\n",
    "               method_arr = method_arr,\n",
    "               method_names = method_names,\n",
    "               downstream_acc_type = downstream_acc_type,\n",
    "               variance_type = variance_type,\n",
    "               p_indices = p_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a70f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n",
    "    if x.size != y.size:\n",
    "        raise ValueError(\"x and y must be the same size\")\n",
    "\n",
    "    cov = np.cov(x, y)\n",
    "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    # Using a special case to obtain the eigenvalues of this\n",
    "    # two-dimensional dataset.\n",
    "    ell_radius_x = np.sqrt(1 + pearson)\n",
    "    ell_radius_y = np.sqrt(1 - pearson)\n",
    "    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      facecolor=facecolor, **kwargs)\n",
    "\n",
    "    # Calculating the standard deviation of x from\n",
    "    # the squareroot of the variance and multiplying\n",
    "    # with the given number of standard deviations.\n",
    "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
    "    mean_x = np.mean(x)\n",
    "\n",
    "    # calculating the standard deviation of y ...\n",
    "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
    "    mean_y = np.mean(y)\n",
    "\n",
    "    transf = transforms.Affine2D() \\\n",
    "        .rotate_deg(45) \\\n",
    "        .scale(scale_x, scale_y) \\\n",
    "        .translate(mean_x, mean_y)\n",
    "\n",
    "    ellipse.set_transform(transf + ax.transData)\n",
    "    return ax.add_patch(ellipse)\n",
    "\n",
    "def batch_scatterplot_grad_dist(metric_name,\n",
    "                   p_vals, \n",
    "                   dataset_name, \n",
    "                   grad_type,\n",
    "                   method_arr,\n",
    "                   method_names,\n",
    "                   downstream_acc_type,\n",
    "                   variance_type,\n",
    "                   p_indices,\n",
    "                   plot_ellipse):\n",
    "    \n",
    "    plt.rcParams['font.family'] = 'Arial'\n",
    "    plt.rcParams['font.size'] = 20\n",
    "    \n",
    "    result_dir = f'gradient_similarity/{dataset_name}/plots'\n",
    "    if not os.path.exists(result_dir): os.mkdir(result_dir)\n",
    "    \n",
    "    plt.figure(figsize = (20, 10))\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    num_methods = len(method_arr)\n",
    "    \n",
    "    for i, p_index in enumerate(p_indices):\n",
    "        \n",
    "        grad_pts = np.zeros(num_methods)\n",
    "        grad_vars = np.zeros(num_methods)\n",
    "        accs = np.zeros(num_methods)\n",
    "        \n",
    "        for method_index, method in enumerate(method_arr):\n",
    "\n",
    "            method_name = method_names[method_index]\n",
    "\n",
    "            acc_block = np.load(f'gradient_similarity/{dataset_name}/{method}/acc_block.npy')\n",
    "            # acc_block is (p_val x num_tasks) of downstream acc performance\n",
    "            if downstream_acc_type == 'last':\n",
    "                acc_arr = acc_block[:, -1]\n",
    "            elif downstream_acc_type == 'avg':\n",
    "                acc_arr = np.mean(acc_block, axis = -1)\n",
    "            else:\n",
    "                print('invalid downstream acc type')\n",
    "                assert False\n",
    "\n",
    "            # load gradients (num_p, num_runs, num_ideal_models, num_tasks-1, num_grad_files)\n",
    "            grad_block = np.load(f'gradient_similarity/{dataset_name}/{method}/{metric_name}/{grad_type}_gradient_comp.npy')\n",
    "\n",
    "            avg_over_grad_files = np.mean(grad_block, axis = -1)\n",
    "            avg_over_tasks = np.mean(avg_over_grad_files, axis = -1)\n",
    "\n",
    "            if variance_type == 'dataset':\n",
    "                avg_grad_block = avg_over_tasks[:,:,0]\n",
    "                # avg over runs\n",
    "                grad_dists = np.mean(avg_grad_block, axis = -1)\n",
    "                grad_stds = np.std(avg_grad_block, axis = -1)\n",
    "            elif variance_type == 'weight':\n",
    "                avg_grad_block = avg_over_tasks[:,0,:]\n",
    "                # avg over ideal models\n",
    "                grad_dists = np.mean(avg_grad_block, axis = -1)\n",
    "                grad_stds = np.std(avg_grad_block, axis = -1)\n",
    "            elif variance_type == 'all':\n",
    "                grad_dists = np.mean(np.mean(avg_over_tasks, axis = -1), axis = -1)\n",
    "                grad_stds = (np.std(avg_over_tasks[:,:,0], axis = -1)**2 + np.std(avg_over_tasks[:,0,:], axis = -1)**2)**(1/2)\n",
    "\n",
    "            grad_pts[method_index] = grad_dists[p_index]\n",
    "            grad_vars[method_index] = grad_stds[p_index]\n",
    "            accs[method_index] = acc_arr[p_index]\n",
    "        \n",
    "        obj = ax.scatter(accs, grad_pts, label = f'p = {p_vals[p_index]}')\n",
    "        if plot_ellipse:\n",
    "            confidence_ellipse(accs, grad_pts, ax, n_std=1.0, edgecolor=obj.get_ec())\n",
    "        else:\n",
    "            plt.errorbar(accs, grad_pts, yerr = grad_vars, ls = 'none') \n",
    "\n",
    "    plt.xlim(0.2, 1)\n",
    "    plt.ylim(0, 1.1)\n",
    "    plt.title(f'Gradient Similarity vs Downstream Accuracy for Memory Selection Methods for Multiple P-values')\n",
    "    plt.ylabel('Gradient Similarity')\n",
    "    plt.xlabel('Downstream Accuracy')\n",
    "    plt.legend(loc = 'lower left')\n",
    "    if plot_ellipse:\n",
    "        plt.savefig(f'{result_dir}/batch_scatterplot_ellipse.png')\n",
    "    else:\n",
    "        plt.savefig(f'{result_dir}/batch_scatterplot.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3710034",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = 'Cosine Similarity'\n",
    "p_values = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\n",
    "dataset_name = 'mnist_split'\n",
    "grad_type = 'past'\n",
    "method_arr = ['random', 'class_balanced', 'GSS', 'lambda', 'kmeans']\n",
    "method_names = ['Random', \"Class-Balanced Reservoir Sampling\", \"Gradient-based Sample Selection\", \"Lambda\", \"K-means\"]\n",
    "downstream_acc_type = 'last' # can be last or avg\n",
    "variance_type = 'all' # can be dataset, weight, or all\n",
    "p_indices = [0, 1, 3, 5, 6, 8]\n",
    "plot_ellipse = False\n",
    "\n",
    "batch_scatterplot_grad_dist(metric_name = metric_name,\n",
    "               p_vals = p_values, \n",
    "               dataset_name = dataset_name, \n",
    "               grad_type = grad_type,\n",
    "               method_arr = method_arr,\n",
    "               method_names = method_names,\n",
    "               downstream_acc_type = downstream_acc_type,\n",
    "               variance_type = variance_type,\n",
    "               p_indices = p_indices,\n",
    "               plot_ellipse = plot_ellipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9616b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = 'Cosine Similarity'\n",
    "p_values = [0.001, 0.002, 0.005, 0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1]\n",
    "dataset_name = 'mnist_split'\n",
    "grad_type = 'past'\n",
    "method_arr = ['random', 'class_balanced', 'GSS', 'lambda', 'kmeans']\n",
    "method_names = ['Random', \"Class-Balanced Reservoir Sampling\", \"Gradient-based Sample Selection\", \"Lambda\", \"K-means\"]\n",
    "downstream_acc_type = 'last' # can be last or avg\n",
    "variance_type = 'all' # can be dataset, weight, or all\n",
    "p_indices = [0, 1, 3, 5, 6, 8]\n",
    "plot_ellipse = True\n",
    "\n",
    "batch_scatterplot_grad_dist(metric_name = metric_name,\n",
    "               p_vals = p_values, \n",
    "               dataset_name = dataset_name, \n",
    "               grad_type = grad_type,\n",
    "               method_arr = method_arr,\n",
    "               method_names = method_names,\n",
    "               downstream_acc_type = downstream_acc_type,\n",
    "               variance_type = variance_type,\n",
    "               p_indices = p_indices,\n",
    "               plot_ellipse = plot_ellipse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3036e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

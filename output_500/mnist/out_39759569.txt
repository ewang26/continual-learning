Using CUDA
Running experiment mnist:
Results are stored in: output_500/mnist
with hyperparameters {'p': 0.01, 'T': 5, 'learning_rate': 0.01, 'batch_size': 60, 'num_centroids': 4, 'model_training_epoch': 10, 'early_stopping_threshold': 0.1, 'random_seed': 2, 'class_balanced': True, 'exp_name': 'mnist'}


task 0, classes 0, 1
task 1, classes 2, 3
task 2, classes 4, 5
task 3, classes 6, 7
task 4, classes 8, 9
Training models M1 and M2
Training model M1
epoch 1 train loss: 45267.441, val loss: 4998.728, train acc: 0.128, val acc: 0.121
diff inf
epoch 2 train loss: 44963.326, val loss: 4987.242, train acc: 0.127, val acc: 0.141
diff 11.486573921059971
epoch 3 train loss: 44945.394, val loss: 4994.338, train acc: 0.131, val acc: 0.124
diff 7.095926058770601
epoch 4 train loss: 44950.375, val loss: 4996.264, train acc: 0.125, val acc: 0.124
diff 1.9266703285684343
epoch 5 train loss: 44949.168, val loss: 4990.326, train acc: 0.129, val acc: 0.141
diff 5.938127112022812
epoch 6 train loss: 44938.612, val loss: 5000.968, train acc: 0.127, val acc: 0.124
diff 10.641188960019463
epoch 7 train loss: 44952.248, val loss: 4991.362, train acc: 0.129, val acc: 0.141
diff 9.605721451169302
epoch 8 train loss: 44938.163, val loss: 4994.234, train acc: 0.131, val acc: 0.141
diff 2.872306275871779
epoch 9 train loss: 44945.088, val loss: 4994.460, train acc: 0.128, val acc: 0.121
diff 0.225575285299783
epoch 10 train loss: 44943.481, val loss: 4989.413, train acc: 0.130, val acc: 0.124
diff 5.047178466615151
Training model M2
epoch 1 train loss: 10417.367, val loss: 932.491, train acc: 0.888, val acc: 0.908
diff inf
epoch 2 train loss: 6463.324, val loss: 817.346, train acc: 0.930, val acc: 0.925
diff 115.14490744891987
epoch 3 train loss: 5269.765, val loss: 671.859, train acc: 0.942, val acc: 0.938
diff 145.4874337891673
epoch 4 train loss: 5043.901, val loss: 705.140, train acc: 0.944, val acc: 0.936
diff 33.28107582110613
epoch 5 train loss: 4908.450, val loss: 636.568, train acc: 0.946, val acc: 0.949
diff 68.57233110421078
epoch 6 train loss: 4205.726, val loss: 565.425, train acc: 0.954, val acc: 0.949
diff 71.14247004308129
epoch 7 train loss: 4930.608, val loss: 543.120, train acc: 0.947, val acc: 0.954
diff 22.305487397927664
epoch 8 train loss: 3487.729, val loss: 486.726, train acc: 0.960, val acc: 0.958
diff 56.39422094413891
epoch 9 train loss: 3418.309, val loss: 506.520, train acc: 0.961, val acc: 0.952
diff 19.794815517099835
epoch 10 train loss: 47665.187, val loss: 6944.844, train acc: 0.322, val acc: 0.101
diff 6438.323485650157
Training model M3
epoch 1 train loss: 1982.779, val loss: 9833.401, train acc: 0.915, val acc: 0.326
diff inf
epoch 2 train loss: 1091.875, val loss: 4971.035, train acc: 0.945, val acc: 0.555
diff 4862.365515997101
epoch 3 train loss: 929.130, val loss: 5281.505, train acc: 0.954, val acc: 0.539
diff 310.46980483708467
epoch 4 train loss: 1227.561, val loss: 5991.037, train acc: 0.949, val acc: 0.559
diff 709.5318652064334
epoch 5 train loss: 748.658, val loss: 5365.591, train acc: 0.963, val acc: 0.499
diff 625.4463519577594
epoch 6 train loss: 618.305, val loss: 5777.504, train acc: 0.966, val acc: 0.623
diff 411.91387545229463
epoch 7 train loss: 552.915, val loss: 5109.545, train acc: 0.971, val acc: 0.648
diff 667.9595817884856
epoch 8 train loss: 744.977, val loss: 7841.420, train acc: 0.965, val acc: 0.583
diff 2731.875494769595
epoch 9 train loss: 648.524, val loss: 5535.476, train acc: 0.968, val acc: 0.604
diff 2305.9443735571076
epoch 10 train loss: 613.806, val loss: 7566.611, train acc: 0.970, val acc: 0.623
diff 2031.1350870308006
Training model M3
epoch 1 train loss: 580.719, val loss: 10324.233, train acc: 0.973, val acc: 0.542
diff inf
epoch 2 train loss: 506.630, val loss: 17293.762, train acc: 0.975, val acc: 0.514
diff 6969.52934371214
epoch 3 train loss: 464.492, val loss: 12946.573, train acc: 0.979, val acc: 0.520
diff 4347.188651380207
epoch 4 train loss: 204.573, val loss: 16169.814, train acc: 0.988, val acc: 0.518
diff 3223.2405920621313
epoch 5 train loss: 253.881, val loss: 17330.884, train acc: 0.987, val acc: 0.517
diff 1161.0695333619933
epoch 6 train loss: 295.693, val loss: 26174.229, train acc: 0.985, val acc: 0.508
diff 8843.345516494679
epoch 7 train loss: 686.809, val loss: 47034.530, train acc: 0.973, val acc: 0.544
diff 20860.3008138064
epoch 8 train loss: 410.530, val loss: 21459.805, train acc: 0.982, val acc: 0.511
diff 25574.725071882953
epoch 9 train loss: 300.781, val loss: 25335.305, train acc: 0.985, val acc: 0.525
diff 3875.500638610396
epoch 10 train loss: 334.183, val loss: 27038.572, train acc: 0.983, val acc: 0.471
diff 1703.266858161609
Training model M3
epoch 1 train loss: 1358.200, val loss: 8934.944, train acc: 0.955, val acc: 0.515
diff inf
epoch 2 train loss: 1469.447, val loss: 11714.342, train acc: 0.949, val acc: 0.598
diff 2779.3979855755533
epoch 3 train loss: 721.851, val loss: 6135.467, train acc: 0.969, val acc: 0.617
diff 5578.8750561304905
epoch 4 train loss: 769.656, val loss: 6707.562, train acc: 0.970, val acc: 0.570
diff 572.0956911248459
epoch 5 train loss: 510.814, val loss: 7615.177, train acc: 0.974, val acc: 0.599
diff 907.6145288707203
epoch 6 train loss: 393.281, val loss: 11210.655, train acc: 0.981, val acc: 0.587
diff 3595.4778987570908
epoch 7 train loss: 403.522, val loss: 12581.587, train acc: 0.979, val acc: 0.570
diff 1370.9317781125774
epoch 8 train loss: 358.887, val loss: 9121.894, train acc: 0.982, val acc: 0.636
diff 3459.692420398469
epoch 9 train loss: 346.301, val loss: 5878.508, train acc: 0.981, val acc: 0.616
diff 3243.3860127721973
epoch 10 train loss: 376.398, val loss: 10010.913, train acc: 0.982, val acc: 0.560
diff 4132.405095742773
2
2
2
2
2
2
Training model M3
epoch 1 train loss: 1078.734, val loss: 5171.301, train acc: 0.959, val acc: 0.646
diff inf
epoch 2 train loss: 1265.180, val loss: 140527.341, train acc: 0.963, val acc: 0.427
diff 135356.03953696124
epoch 3 train loss: 1109.645, val loss: 7487.272, train acc: 0.967, val acc: 0.618
diff 133040.06935935852
epoch 4 train loss: 416.775, val loss: 6836.996, train acc: 0.977, val acc: 0.673
diff 650.2751799271064
epoch 5 train loss: 269.264, val loss: 6848.821, train acc: 0.985, val acc: 0.695
diff 11.824968686586544
epoch 6 train loss: 291.695, val loss: 13701.219, train acc: 0.984, val acc: 0.657
diff 6852.397612872955
epoch 7 train loss: 298.428, val loss: 9385.430, train acc: 0.984, val acc: 0.658
diff 4315.7889076245465
epoch 8 train loss: 376.345, val loss: 8412.675, train acc: 0.981, val acc: 0.653
diff 972.7556328158535
epoch 9 train loss: 348.391, val loss: 6821.011, train acc: 0.986, val acc: 0.663
diff 1591.6631921731096
epoch 10 train loss: 464.910, val loss: 12040.618, train acc: 0.978, val acc: 0.633
diff 5219.606807220951
training representation using icarl loss
training representation using icarl loss
training representation using icarl loss
training representation using icarl loss
Training model M3
epoch 1 train loss: 1353.950, val loss: 4560.559, train acc: 0.953, val acc: 0.581
diff inf
epoch 2 train loss: 807.248, val loss: 4981.035, train acc: 0.965, val acc: 0.659
diff 420.47614212964527
epoch 3 train loss: 663.494, val loss: 11798.564, train acc: 0.969, val acc: 0.580
diff 6817.529312583188
epoch 4 train loss: 1731.248, val loss: 25485.010, train acc: 0.951, val acc: 0.558
diff 13686.445902660604
epoch 5 train loss: 1096.732, val loss: 8138.336, train acc: 0.960, val acc: 0.615
diff 17346.67371486566
epoch 6 train loss: 335.473, val loss: 8713.666, train acc: 0.983, val acc: 0.606
diff 575.329715416694
epoch 7 train loss: 259.551, val loss: 10138.807, train acc: 0.985, val acc: 0.631
diff 1425.1410261707424
epoch 8 train loss: 239.253, val loss: 9466.448, train acc: 0.986, val acc: 0.610
diff 672.3587039300928
epoch 9 train loss: 322.795, val loss: 13679.055, train acc: 0.984, val acc: 0.620
diff 4212.60666059699
epoch 10 train loss: 468.896, val loss: 8077.136, train acc: 0.980, val acc: 0.555
diff 5601.919446691661
training representation using replay loss
training representation using replay loss
training representation using replay loss
